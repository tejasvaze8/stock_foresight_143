{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.12.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U torch transformers accelerate datasets\n",
    "!pip install -q flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 09:02:00.055230: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-11 09:02:00.840460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325593e8e1424b7d8132a14d97efd69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"tchen175/llama3.1-8b-newsmtsc\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"cuda\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Determine the sentiment of the following article as -1 (negative), 0 (neutral), or 1 (positive).\n",
    "\n",
    "### Article:\n",
    "{article}\n",
    "\n",
    "### Semantic label:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval our finetuned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_logits(model, tokenizer, dataset, device='cuda'):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    for text in dataset['sentence']:\n",
    "        prompt = alpaca_prompt.format(article = text)\n",
    "\n",
    "        # Prepare input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "\n",
    "        possible_outputs = ['-1', '0', '1']\n",
    "\n",
    "        output_token_ids = [tokenizer(output, add_special_tokens=False).input_ids[0] for output in possible_outputs]\n",
    "        \n",
    "        # Get logits\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        last_token_logits = logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "\n",
    "        # Filter logits for the possible outputs\n",
    "        filtered_logits = last_token_logits[:, output_token_ids]  # Shape: (batch_size, len(possible_outputs))\n",
    "\n",
    "        # Print filtered logits (comment this line for performance)\n",
    "        # print(\"Logits for possible outputs:\", filtered_logits)\n",
    "\n",
    "        # Optionally, convert logits to probabilities\n",
    "        probabilities = torch.softmax(filtered_logits, dim=-1)\n",
    "        # print(\"Probabilities for possible outputs:\", probabilities)  # Commented to avoid excessive I/O\n",
    "\n",
    "        # Get the label with the highest probability\n",
    "        predicted_label_index = torch.argmax(probabilities, dim=-1).item()\n",
    "        predicted_label = possible_outputs[predicted_label_index]\n",
    "        predictions.append(int(predicted_label))\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval GPT4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key:sk-proj-yKHe10Y7zEhFX1sncOFCT3BlbkFJh31ATScEQmYjQRlNpiLg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    base_url=\"https://api.openai.com/v1\",\n",
    "    api_key=api_key#enter api key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_schema = {\n",
    "        \"name\": \"sentiment_analysis\",\n",
    "        \"description\": \"Analyze the sentiment of the given article and return -1 (negative), 0 (neutral), or 1 (positive).\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"article\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The article text to analyze for sentiment.\"\n",
    "                }\n",
    "            },\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"The sentiment of the article: -1 for negative, 0 for neutral, 1 for positive.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"article\", 'sentiment']\n",
    "        },\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def predict_sentiment_with_function_calling(openai_client, dataset):\n",
    "    # Define the function schema\n",
    "    \n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for text in dataset['sentence']:\n",
    "        # Call the OpenAI API with the function schema\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sentiment analysis assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Analyze the sentiment of this article: {text}\"}\n",
    "            ],\n",
    "            functions=[function_schema],\n",
    "            function_call={\"name\": \"sentiment_analysis\"}\n",
    "        )\n",
    "        for i in range(3):\n",
    "            # Parse the function's arguments\n",
    "            try:\n",
    "                function_call_args = response.choices[0].message.function_call.arguments\n",
    "                sentiment_analysis_result = json.loads(function_call_args)  # Safely parse arguments\n",
    "                sentiment = sentiment_analysis_result['sentiment']  # Default to 0 if not provided\n",
    "                predictions.append(sentiment)\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                if i == 2:\n",
    "                    print(sentiment_analysis_result)\n",
    "                    print(\"Error parsing function call arguments:\", e)\n",
    "                    predictions.append(0)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article = \"The stock market is crashing and the economy is in shambles. The unemployment rate is at an all-time high and people are struggling to make ends meet. The government is doing nothing to help and the future looks bleak.\"\n",
    "# eval(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"fhamborg/news_sentiment_newsmtsc\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mention': 'Seth',\n",
       " 'polarity': 0,\n",
       " 'from': 39,\n",
       " 'to': 43,\n",
       " 'sentence': 'Though we do not know what other items Seth may have had in his possession, his watch, phone, wallet and necklace were not stolen.',\n",
       " 'id': 'polusa_v1_4307505_-1_11_Seth_39_43'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict_sentiment_logits(model, tokenizer, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "Error parsing function call arguments: 'sentiment'\n"
     ]
    }
   ],
   "source": [
    "results = predict_sentiment_with_function_calling(openai_client, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = test_data['polarity']\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(actual, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6725\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
