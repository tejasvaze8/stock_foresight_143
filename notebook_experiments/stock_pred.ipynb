{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U torch transformers accelerate datasets\n",
    "!pip install -q flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 10:49:27.194341: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-11 10:49:27.971882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3855508df846ce9c9944db1f2433e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"tchen175/llama3.1-8b-newsmtsc\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"cuda\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Determine the sentiment of the following article as -1 (negative), 0 (neutral), or 1 (positive).\n",
    "\n",
    "### Article:\n",
    "{article}\n",
    "\n",
    "### Semantic label:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval our finetuned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_logits(text, device='cuda'):\n",
    "    \n",
    "    \n",
    "    prompt = alpaca_prompt.format(article = text)\n",
    "\n",
    "    # Prepare input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "\n",
    "    possible_outputs = ['-1', '0', '1']\n",
    "\n",
    "    output_token_ids = [tokenizer(output, add_special_tokens=False).input_ids[0] for output in possible_outputs]\n",
    "    \n",
    "    # Get logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    last_token_logits = logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "\n",
    "    # Filter logits for the possible outputs\n",
    "    filtered_logits = last_token_logits[:, output_token_ids]  # Shape: (batch_size, len(possible_outputs))\n",
    "\n",
    "    # Print filtered logits (comment this line for performance)\n",
    "    # print(\"Logits for possible outputs:\", filtered_logits)\n",
    "\n",
    "    # Optionally, convert logits to probabilities\n",
    "    probabilities = torch.softmax(filtered_logits, dim=-1)\n",
    "    # print(\"Probabilities for possible outputs:\", probabilities)  # Commented to avoid excessive I/O\n",
    "\n",
    "    # Get the label with the highest probability\n",
    "    predicted_label_index = torch.argmax(probabilities, dim=-1).item()\n",
    "    predicted_label = possible_outputs[predicted_label_index]\n",
    "\n",
    "    \n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58 AAPL.txt\n",
      "0.7333333333333333 AI.txt\n",
      "0.64 AMZN.txt\n",
      "0.7 AVGO.txt\n",
      "0.52 INTC.txt\n",
      "0.5 LCID.txt\n",
      "0.26 NVDA.txt\n",
      "0.6666666666666666 PEP.txt\n",
      "0.56 PLTR.txt\n",
      "0.5869565217391305 TGT.txt\n",
      "0.6 UBER.txt\n",
      "0.24 UNH.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "article_path = \"../stock_articles/\" \n",
    "\n",
    "result = {}\n",
    "\n",
    "for filename in os.listdir(article_path):\n",
    "\n",
    "    if filename.endswith(\".txt\"):\n",
    "        # Build the full file path\n",
    "        file_path = os.path.join(article_path, filename)\n",
    "        \n",
    "        # Open and read the text file line by line\n",
    "        with open(file_path, 'r') as file:\n",
    "            score, total = 0, 0\n",
    "\n",
    "            for line in file:\n",
    "                # Run invoke on each line\n",
    "                sentiment = predict_sentiment_logits(line)\n",
    "                if sentiment == '1':\n",
    "                    score += 1\n",
    "                elif sentiment == '0':\n",
    "                    score += 0.5\n",
    "\n",
    "                total += 1\n",
    "            print(score/total, filename)\n",
    "            result[filename] = score/total\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
