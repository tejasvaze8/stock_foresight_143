{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.1.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (2023.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.26.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"fhamborg/news_sentiment_newsmtsc\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae24a9c25a0a44ae9e9928d5111e7daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859fa2486fdd47a08d3c5983950e6209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea6bff4b365468da6bcfa6150043d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58b9bd82f7b423883d5fe8673408b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Replace 'bert-base-uncased' with your specific model's name\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2bbee8c8f546d7bc600031dc7df109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8739 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['sentence'],   # Adjust key based on your dataset\n",
    "        padding=\"max_length\",  # Pad to max_length (helps batching)\n",
    "        truncation=True,       # Truncate sequences longer than max_length\n",
    "        max_length=512         # Set a suitable max_length\n",
    "        \n",
    "    )\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenized_dataset['train']\n",
    "test_data = tokenized_dataset['test']\n",
    "validation_data = tokenized_dataset['validation']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'polarity'])\n",
    "\n",
    "test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'polarity'])\n",
    "\n",
    "validation_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, -1, -1,  ...,  0,  1, -1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the RNN model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size: int, \n",
    "                 hidden_size: int = 128,\n",
    "                 num_layers: int = 2,\n",
    "                 num_classes: int = 2,\n",
    "                 dropout_rate: float = 0.3):\n",
    "        \n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, \n",
    "                          hidden_size, \n",
    "                          num_layers, \n",
    "                          batch_first=True, \n",
    "                          dropout=dropout_rate if num_layers > 1 else 0.0)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, \n",
    "                         x.size(0), \n",
    "                         self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate through RNN\n",
    "        out, _ = self.rnn(x, h0)  # out: [batch_size, seq_length, hidden_size]\n",
    "        \n",
    "        # Get the output from the last time step\n",
    "        out = out[:, -1, :]  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(out)  # [batch_size, num_classes]\n",
    "        return out\n",
    "\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int = 128, num_layers: int = 2,\n",
    "                 num_classes: int = 2, dropout_rate: float = 0.3):\n",
    "        \"\"\"\n",
    "        GRU Model for sequence classification\n",
    "        \"\"\"\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        # Determine number of directions\n",
    "        \n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "        # Dropout and normalization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        fc_input_size = hidden_size\n",
    "        self.fc1 = nn.Linear(fc_input_size, fc_input_size // 2)\n",
    "        self.fc2 = nn.Linear(fc_input_size // 2, num_classes)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(fc_input_size // 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the GRU model\n",
    "        \"\"\"\n",
    "        # Initialize hidden state\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.gru.num_layers, batch_size, \n",
    "            self.gru.hidden_size, device=x.device\n",
    "        \n",
    "        )\n",
    "        \n",
    "        # GRU processing\n",
    "        out, hidden = self.gru(x, h0)\n",
    "        \n",
    "        # Extract the last hidden state\n",
    "     \n",
    "        hidden = hidden[-1]\n",
    "        \n",
    "        # Apply dropout and fully connected layers\n",
    "        x = self.dropout(hidden)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int = 128,num_layers: int = 3,\n",
    "                 num_classes: int = 3,dropout_rate: float = 0.3):\n",
    "        \n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate,\n",
    "        )\n",
    "        \n",
    "        # Dropout and normalization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        fc_input_size = hidden_size\n",
    "        self.fc1 = nn.Linear(fc_input_size, fc_input_size // 2)\n",
    "        self.fc = nn.Linear(fc_input_size // 2, fc_input_size // 2)\n",
    "        self.fc2 = nn.Linear(fc_input_size // 2, num_classes)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(fc_input_size // 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM model\n",
    "        \n",
    "        \"\"\"\n",
    "        # Initialize hidden and cell states\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.lstm.num_layers, batch_size, \n",
    "            self.lstm.hidden_size, device=x.device)\n",
    "        c0 = torch.zeros(self.lstm.num_layers, batch_size, \n",
    "            self.lstm.hidden_size, device=x.device)\n",
    "        \n",
    "        # LSTM processing\n",
    "        _, (hidden, _) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        hidden = hidden[-1]\n",
    "        \n",
    "        # Apply dropout and fully connected layers\n",
    "        x = self.dropout(hidden)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        # x = F.relu(self.fc(x))\n",
    "        # x = self.layer_norm(x)\n",
    "        # x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "parameters = {\n",
    "    'input_size': train_data['input_ids'].shape[1],\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 5,\n",
    "    'num_classes': 3,\n",
    "    'dropout_rate': 0.3,\n",
    "    \n",
    "}\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN = RNNModel(**parameters)\n",
    "LSTM = LSTMModel(**parameters)\n",
    "GRU = GRUModel(**parameters)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model):\n",
    "   optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "   \n",
    "   # Training loop\n",
    "\n",
    "   model.train()  # Set model to training mode\n",
    "\n",
    "   for epoch in range(num_epochs):\n",
    "      \n",
    "      train_inputs = train_data['input_ids'].float()\n",
    "      train_labels = train_data['polarity']\n",
    "\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(train_inputs.unsqueeze(1))  # Shape: [batch_size, seq_length, input_size]\n",
    "\n",
    "      train_labels[train_labels == -1] = 2\n",
    "      outputs[outputs == -1] = 2\n",
    "      \n",
    "\n",
    "      # Calculate the loss\n",
    "      loss = criterion(outputs, train_labels)\n",
    "\n",
    "      # Backward pass and optimization\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Calculate accuracy\n",
    "      _, predicted = torch.max(outputs, dim=1)\n",
    "      correct = (predicted == train_labels).sum().item()\n",
    "      accuracy = correct / train_labels.size(0)\n",
    "\n",
    "      print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "   # Evaluation loop\n",
    "   # Evaluation\n",
    "   model.eval()  # Set model to evaluation mode\n",
    "   with torch.no_grad():\n",
    "      test_inputs = test_data['input_ids'].float()\n",
    "      test_labels = test_data['polarity']\n",
    "\n",
    "      test_labels[test_labels == -1] = 2\n",
    "      \n",
    "      \n",
    "      test_outputs = model(test_inputs.unsqueeze(1))\n",
    "      test_outputs[test_outputs == -1] = 2\n",
    "      print(test_outputs)\n",
    "      _, predicted = torch.max(test_outputs, 1)\n",
    "      \n",
    "      \n",
    "      accuracy = (predicted == test_labels).sum().item() / test_labels.size(0)\n",
    "      print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1,  0,  1])\n"
     ]
    }
   ],
   "source": [
    "print(train_data['polarity'].unique())  # Check unique values in labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.0785, Accuracy: 0.4084\n",
      "Epoch [2/20], Loss: 1.0784, Accuracy: 0.4101\n",
      "Epoch [3/20], Loss: 1.0803, Accuracy: 0.4082\n",
      "Epoch [4/20], Loss: 1.0776, Accuracy: 0.4143\n",
      "Epoch [5/20], Loss: 1.0793, Accuracy: 0.4089\n",
      "Epoch [6/20], Loss: 1.0791, Accuracy: 0.4121\n",
      "Epoch [7/20], Loss: 1.0777, Accuracy: 0.4158\n",
      "Epoch [8/20], Loss: 1.0789, Accuracy: 0.4083\n",
      "Epoch [9/20], Loss: 1.0778, Accuracy: 0.4173\n",
      "Epoch [10/20], Loss: 1.0762, Accuracy: 0.4188\n",
      "Epoch [11/20], Loss: 1.0766, Accuracy: 0.4155\n",
      "Epoch [12/20], Loss: 1.0769, Accuracy: 0.4169\n",
      "Epoch [13/20], Loss: 1.0782, Accuracy: 0.4101\n",
      "Epoch [14/20], Loss: 1.0779, Accuracy: 0.4131\n",
      "Epoch [15/20], Loss: 1.0752, Accuracy: 0.4165\n",
      "Epoch [16/20], Loss: 1.0752, Accuracy: 0.4179\n",
      "Epoch [17/20], Loss: 1.0773, Accuracy: 0.4165\n",
      "Epoch [18/20], Loss: 1.0748, Accuracy: 0.4156\n",
      "Epoch [19/20], Loss: 1.0787, Accuracy: 0.4121\n",
      "Epoch [20/20], Loss: 1.0763, Accuracy: 0.4170\n",
      "tensor([[-0.2434, -0.1919,  0.2016],\n",
      "        [ 0.2463, -0.2491, -0.0076],\n",
      "        [ 0.2767, -0.2145, -0.0008],\n",
      "        ...,\n",
      "        [ 0.0404, -0.1920,  0.1264],\n",
      "        [ 0.0404, -0.1920,  0.1264],\n",
      "        [-0.0475, -0.0013, -0.0229]])\n",
      "Test Accuracy: 0.4234\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.2293, Accuracy: 0.2964\n",
      "Epoch [2/20], Loss: 1.1691, Accuracy: 0.3238\n",
      "Epoch [3/20], Loss: 1.1382, Accuracy: 0.3401\n",
      "Epoch [4/20], Loss: 1.1409, Accuracy: 0.3609\n",
      "Epoch [5/20], Loss: 1.1536, Accuracy: 0.3602\n",
      "Epoch [6/20], Loss: 1.1513, Accuracy: 0.3687\n",
      "Epoch [7/20], Loss: 1.1511, Accuracy: 0.3708\n",
      "Epoch [8/20], Loss: 1.1428, Accuracy: 0.3610\n",
      "Epoch [9/20], Loss: 1.1358, Accuracy: 0.3618\n",
      "Epoch [10/20], Loss: 1.1259, Accuracy: 0.3632\n",
      "Epoch [11/20], Loss: 1.1229, Accuracy: 0.3603\n",
      "Epoch [12/20], Loss: 1.1268, Accuracy: 0.3491\n",
      "Epoch [13/20], Loss: 1.1265, Accuracy: 0.3423\n",
      "Epoch [14/20], Loss: 1.1270, Accuracy: 0.3350\n",
      "Epoch [15/20], Loss: 1.1248, Accuracy: 0.3450\n",
      "Epoch [16/20], Loss: 1.1300, Accuracy: 0.3350\n",
      "Epoch [17/20], Loss: 1.1213, Accuracy: 0.3384\n",
      "Epoch [18/20], Loss: 1.1224, Accuracy: 0.3391\n",
      "Epoch [19/20], Loss: 1.1185, Accuracy: 0.3472\n",
      "Epoch [20/20], Loss: 1.1206, Accuracy: 0.3455\n",
      "tensor([[ 0.2540, -0.0115,  0.3126],\n",
      "        [ 0.2618, -0.0159,  0.3067],\n",
      "        [ 0.2599, -0.0125,  0.3102],\n",
      "        ...,\n",
      "        [ 0.2610, -0.0133,  0.3088],\n",
      "        [ 0.2610, -0.0133,  0.3088],\n",
      "        [ 0.2642, -0.0160,  0.3063]])\n",
      "Test Accuracy: 0.3674\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.3013, Accuracy: 0.3744\n",
      "Epoch [2/20], Loss: 1.1653, Accuracy: 0.3600\n",
      "Epoch [3/20], Loss: 1.1358, Accuracy: 0.3490\n",
      "Epoch [4/20], Loss: 1.1574, Accuracy: 0.3440\n",
      "Epoch [5/20], Loss: 1.1621, Accuracy: 0.3400\n",
      "Epoch [6/20], Loss: 1.1508, Accuracy: 0.3440\n",
      "Epoch [7/20], Loss: 1.1358, Accuracy: 0.3486\n",
      "Epoch [8/20], Loss: 1.1255, Accuracy: 0.3410\n",
      "Epoch [9/20], Loss: 1.1196, Accuracy: 0.3600\n",
      "Epoch [10/20], Loss: 1.1190, Accuracy: 0.3606\n",
      "Epoch [11/20], Loss: 1.1188, Accuracy: 0.3646\n",
      "Epoch [12/20], Loss: 1.1209, Accuracy: 0.3603\n",
      "Epoch [13/20], Loss: 1.1185, Accuracy: 0.3719\n",
      "Epoch [14/20], Loss: 1.1212, Accuracy: 0.3695\n",
      "Epoch [15/20], Loss: 1.1151, Accuracy: 0.3772\n",
      "Epoch [16/20], Loss: 1.1130, Accuracy: 0.3681\n",
      "Epoch [17/20], Loss: 1.1110, Accuracy: 0.3688\n",
      "Epoch [18/20], Loss: 1.1077, Accuracy: 0.3725\n",
      "Epoch [19/20], Loss: 1.1099, Accuracy: 0.3633\n",
      "Epoch [20/20], Loss: 1.1045, Accuracy: 0.3674\n",
      "tensor([[-0.0799, -0.3015, -0.0253],\n",
      "        [-0.0428, -0.3625, -0.0571],\n",
      "        [ 0.0605, -0.3487, -0.0947],\n",
      "        ...,\n",
      "        [-0.0650, -0.3325, -0.0315],\n",
      "        [-0.0650, -0.3325, -0.0315],\n",
      "        [ 0.0295, -0.3036, -0.1276]])\n",
      "Test Accuracy: 0.3985\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
