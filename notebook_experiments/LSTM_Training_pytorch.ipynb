{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p99C7aICVvEX"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U datasets torch imbalanced-learn transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXF1KtZlVvEa",
        "outputId": "974f009c-476b-4912-c0fe-83300a79b473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"takala/financial_phrasebank\", \"sentences_50agree\", trust_remote_code = True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egIX-gs09aSd",
        "outputId": "93ae1051-75a9-4445-c283-42617200b53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'label'],\n",
              "        num_rows: 4846\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOGIlqGEVvEf"
      },
      "source": [
        "## Preprocess using Pretrained tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZX79R5RVvEf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Replace 'bert-base-uncased' with your specific model's name\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGyI5yy8VvEf"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['sentence'],   # Adjust key based on your dataset\n",
        "        padding=\"max_length\",  # Pad to max_length (helps batching)\n",
        "        truncation=True,       # Truncate sequences longer than max_length\n",
        "        max_length=128         # Set a suitable max_length\n",
        "\n",
        "    )\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset = tokenized_dataset['train'].train_test_split(test_size=0.2)\n",
        "\n",
        "train_data = split_dataset['train']\n",
        "test_data = split_dataset['test']\n",
        "\n",
        "\n",
        "\n",
        "# Set the format for PyTorch tensors\n",
        "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
      ],
      "metadata": {
        "id": "2w2wwZsl8Ucn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNHeX-wwVvEf",
        "outputId": "f633c90d-ba20-428b-d71c-6a223d316e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New balanced distribution:\n",
            "Negative (Label 0): 488 reviews\n",
            "Neutral (Label 1): 488 reviews\n",
            "Positive (Label 2): 488 reviews\n",
            "\n",
            "Original dataset size: 3876\n",
            "Balanced dataset size: 1464\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "# First, separate examples by label\n",
        "label_0_indices = [i for i, label in enumerate(train_data['label']) if label == 0]\n",
        "label_1_indices = [i for i, label in enumerate(train_data['label']) if label == 1]\n",
        "label_2_indices = [i for i, label in enumerate(train_data['label']) if label == 2]\n",
        "\n",
        "# Find the size of the smallest class\n",
        "min_size = min(len(label_0_indices), len(label_1_indices), len(label_2_indices))\n",
        "\n",
        "# Randomly sample from each class to get equal numbers\n",
        "random.seed(42)  # for reproducibility\n",
        "balanced_0_indices = random.sample(label_0_indices, min_size)\n",
        "balanced_1_indices = random.sample(label_1_indices, min_size)\n",
        "balanced_2_indices = random.sample(label_2_indices, min_size)\n",
        "\n",
        "# Combine all indices and create new balanced dataset\n",
        "all_balanced_indices = balanced_0_indices + balanced_1_indices + balanced_2_indices\n",
        "balanced_train_data = train_data.select(all_balanced_indices)\n",
        "\n",
        "# Verify the new distribution\n",
        "unique_labels, counts = torch.unique(balanced_train_data['label'], return_counts=True)\n",
        "print(\"\\nNew balanced distribution:\")\n",
        "for label, count in zip(unique_labels, counts):\n",
        "    sentiment = \"Negative\" if label == 0 else \"Neutral\" if label == 1 else \"Positive\"\n",
        "    print(f\"{sentiment} (Label {label}): {count.item()} reviews\")\n",
        "\n",
        "# Print total sizes\n",
        "print(f\"\\nOriginal dataset size: {len(train_data)}\")\n",
        "print(f\"Balanced dataset size: {len(balanced_train_data)}\")\n",
        "\n",
        "train_data = balanced_train_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m944A5skVvEg",
        "outputId": "1c701a44-9be2-4336-d3fa-a9868ccbfaa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 labels after shuffling: [2, 2, 0, 0, 2, 2, 0, 2, 2, 1]\n"
          ]
        }
      ],
      "source": [
        "# Get all indices in random order\n",
        "indices = torch.randperm(len(train_data)).tolist()\n",
        "\n",
        "# Shuffle the dataset using these indices\n",
        "train_data = train_data.select(indices)\n",
        "\n",
        "# Verify it's shuffled by printing first 10 labels\n",
        "print(\"First 10 labels after shuffling:\", train_data['label'][:10].tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "rA18qONYZpa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "import random\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=256, hidden_size=128, num_layers=2, num_classes=3, dropout_rate=0.3):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # LSTM Layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout_rate if num_layers > 1 else 0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
        "\n",
        "        # Dropout and normalization\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    def attention_net(self, lstm_output, attention_mask):\n",
        "        # Calculate attention scores\n",
        "        attention_weights = self.attention(lstm_output)\n",
        "        attention_weights = attention_weights.squeeze(-1)\n",
        "\n",
        "        # Apply attention mask\n",
        "        attention_weights = attention_weights.masked_fill(~attention_mask, float('-inf'))\n",
        "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
        "\n",
        "        # Apply attention to LSTM output\n",
        "        context = torch.bmm(attention_weights.unsqueeze(1), lstm_output)\n",
        "        return context.squeeze(1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Embedding\n",
        "        embedded = self.embedding(input_ids)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_output, _ = self.lstm(embedded)\n",
        "\n",
        "        # Attention\n",
        "        context = self.attention_net(lstm_output, attention_mask.bool())\n",
        "\n",
        "        # Fully connected layers with residual connections\n",
        "        x = self.dropout(context)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    return total_loss / len(train_loader), total_correct / total_samples\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return (total_loss / len(test_loader),\n",
        "            classification_report(true_labels, predictions, digits=4))\n",
        "\n",
        "# Training setup\n",
        "def prepare_dataloaders(train_data, test_data, batch_size=32):\n",
        "    train_dataset = TensorDataset(\n",
        "        train_data['input_ids'],\n",
        "        train_data['attention_mask'],\n",
        "        train_data['label']\n",
        "    )\n",
        "\n",
        "    test_dataset = TensorDataset(\n",
        "        test_data['input_ids'],\n",
        "        test_data['attention_mask'],\n",
        "        test_data['label']\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Separate model config from training config\n",
        "model_config = {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'embedding_dim': 512,\n",
        "    'hidden_size': 128,\n",
        "    'num_layers': 1,\n",
        "    'num_classes': 3,\n",
        "    'dropout_rate': 0.5\n",
        "}\n",
        "\n",
        "training_config = {\n",
        "    'learning_rate': 0.0005,\n",
        "    'batch_size': 32,\n",
        "    'num_epochs': 10\n",
        "}\n",
        "# Initialize model and training components\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = LSTMModel(**model_config).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=training_config['learning_rate'])\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
        "\n",
        "# Prepare data loaders\n",
        "train_loader, test_loader = prepare_dataloaders(train_data, test_data, training_config['batch_size'])\n"
      ],
      "metadata": {
        "id": "Q-3YjiZPFskV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Loop\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(training_config['num_epochs']):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_report = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{training_config[\"num_epochs\"]}:')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "    print(f'Val Loss: {val_loss:.4f}')\n",
        "    print('Validation Report:')\n",
        "    print(val_report)\n",
        "    print('-' * 60)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZftJQ2iwTfoG",
        "outputId": "b5c0ad64-ac61-4258-907a-4eb430bd23b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10:\n",
            "Train Loss: 1.1453, Train Acc: 0.3921\n",
            "Val Loss: 0.9226\n",
            "Validation Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3471    0.5086    0.4126       116\n",
            "           1     0.7258    0.7745    0.7494       581\n",
            "           2     0.4389    0.2894    0.3488       273\n",
            "\n",
            "    accuracy                         0.6062       970\n",
            "   macro avg     0.5039    0.5242    0.5036       970\n",
            "weighted avg     0.5998    0.6062    0.5964       970\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 2/10:\n",
            "Train Loss: 0.9468, Train Acc: 0.5376\n",
            "Val Loss: 0.8891\n",
            "Validation Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3744    0.6293    0.4695       116\n",
            "           1     0.7962    0.6523    0.7171       581\n",
            "           2     0.4281    0.4689    0.4476       273\n",
            "\n",
            "    accuracy                         0.5979       970\n",
            "   macro avg     0.5329    0.5835    0.5447       970\n",
            "weighted avg     0.6422    0.5979    0.6116       970\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 3/10:\n",
            "Train Loss: 0.8073, Train Acc: 0.6182\n",
            "Val Loss: 0.8252\n",
            "Validation Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3936    0.6379    0.4868       116\n",
            "           1     0.7816    0.7022    0.7398       581\n",
            "           2     0.4923    0.4689    0.4803       273\n",
            "\n",
            "    accuracy                         0.6289       970\n",
            "   macro avg     0.5558    0.6030    0.5690       970\n",
            "weighted avg     0.6538    0.6289    0.6365       970\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 4/10:\n",
            "Train Loss: 0.6194, Train Acc: 0.7363\n",
            "Val Loss: 0.8971\n",
            "Validation Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4479    0.6293    0.5233       116\n",
            "           1     0.8232    0.5370    0.6500       581\n",
            "           2     0.4393    0.6886    0.5364       273\n",
            "\n",
            "    accuracy                         0.5907       970\n",
            "   macro avg     0.5701    0.6183    0.5699       970\n",
            "weighted avg     0.6703    0.5907    0.6029       970\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 5/10:\n",
            "Train Loss: 0.4459, Train Acc: 0.8292\n",
            "Val Loss: 0.9274\n",
            "Validation Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4213    0.7845    0.5482       116\n",
            "           1     0.8087    0.6403    0.7147       581\n",
            "           2     0.5340    0.5751    0.5538       273\n",
            "\n",
            "    accuracy                         0.6392       970\n",
            "   macro avg     0.5880    0.6666    0.6056       970\n",
            "weighted avg     0.6851    0.6392    0.6495       970\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 6/10:\n",
            "Train Loss: 0.3016, Train Acc: 0.8955\n",
            "Val Loss: 0.9222\n",
            "Validation Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5060    0.7328    0.5986       116\n",
            "           1     0.7837    0.7298    0.7558       581\n",
            "           2     0.5670    0.5421    0.5543       273\n",
            "\n",
            "    accuracy                         0.6773       970\n",
            "   macro avg     0.6189    0.6682    0.6362       970\n",
            "weighted avg     0.6895    0.6773    0.6803       970\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 7/10:\n",
            "Train Loss: 0.1829, Train Acc: 0.9440\n",
            "Val Loss: 1.0919\n",
            "Validation Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4607    0.7069    0.5578       116\n",
            "           1     0.7995    0.6041    0.6882       581\n",
            "           2     0.4986    0.6447    0.5623       273\n",
            "\n",
            "    accuracy                         0.6278       970\n",
            "   macro avg     0.5863    0.6519    0.6028       970\n",
            "weighted avg     0.6743    0.6278    0.6372       970\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 8/10:\n",
            "Train Loss: 0.1192, Train Acc: 0.9645\n",
            "Val Loss: 1.1460\n",
            "Validation Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5325    0.7069    0.6074       116\n",
            "           1     0.8083    0.6386    0.7135       581\n",
            "           2     0.5126    0.6703    0.5810       273\n",
            "\n",
            "    accuracy                         0.6557       970\n",
            "   macro avg     0.6178    0.6719    0.6339       970\n",
            "weighted avg     0.6921    0.6557    0.6635       970\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 9/10:\n",
            "Train Loss: 0.0834, Train Acc: 0.9816\n",
            "Val Loss: 1.2670\n",
            "Validation Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4940    0.7155    0.5845       116\n",
            "           1     0.8058    0.6644    0.7283       581\n",
            "           2     0.5294    0.6264    0.5738       273\n",
            "\n",
            "    accuracy                         0.6598       970\n",
            "   macro avg     0.6098    0.6688    0.6289       970\n",
            "weighted avg     0.6908    0.6598    0.6676       970\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 10/10:\n",
            "Train Loss: 0.0569, Train Acc: 0.9870\n",
            "Val Loss: 1.3523\n",
            "Validation Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4670    0.7328    0.5705       116\n",
            "           1     0.8086    0.6472    0.7189       581\n",
            "           2     0.5263    0.6227    0.5705       273\n",
            "\n",
            "    accuracy                         0.6505       970\n",
            "   macro avg     0.6007    0.6675    0.6200       970\n",
            "weighted avg     0.6883    0.6505    0.6594       970\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Model"
      ],
      "metadata": {
        "id": "4IHOtVBDZxGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    return np.array(all_predictions), np.array(all_labels), np.array(all_probs)\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "def get_example_predictions(test_data, predictions, probabilities, n=10):\n",
        "    examples = []\n",
        "    for i in range(len(predictions)):\n",
        "        examples.append({\n",
        "            'text': tokenizer.decode(test_data['input_ids'][i]),\n",
        "            'true_label': test_data['label'][i].item(),\n",
        "            'predicted_label': predictions[i],\n",
        "            'confidence': max(probabilities[i]) * 100\n",
        "        })\n",
        "\n",
        "    # Sort by confidence\n",
        "    examples.sort(key=lambda x: x['confidence'], reverse=True)\n",
        "    return examples[:n]\n",
        "\n",
        "# Load the best model\n",
        "model = LSTMModel(**model_config).to(device)\n",
        "model.load_state_dict(torch.load('/content/best_model.pt'))\n",
        "model.to(device)\n",
        "\n",
        "# Get predictions\n",
        "predictions, true_labels, probabilities = evaluate_model(model, test_loader, device)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(true_labels, predictions, digits=4))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(true_labels, predictions)\n",
        "\n",
        "# Print some example predictions\n",
        "print(\"\\nExample Predictions:\")\n",
        "examples = get_example_predictions(test_data, predictions, probabilities)\n",
        "for i, example in enumerate(examples, 1):\n",
        "    sentiment = \"Negative\" if example['predicted_label'] == 0 else \"Neutral\" if example['predicted_label'] == 1 else \"Positive\"\n",
        "    correct = \"✓\" if example['predicted_label'] == example['true_label'] else \"✗\"\n",
        "\n",
        "    print(f\"\\nExample {i} [{correct}]\")\n",
        "    print(f\"Text: {example['text']}\")\n",
        "    print(f\"True Label: {example['true_label']}\")\n",
        "    print(f\"Predicted: {sentiment} (Confidence: {example['confidence']:.2f}%)\")\n",
        "\n",
        "# Calculate per-class accuracy\n",
        "for i in range(3):\n",
        "    mask = true_labels == i\n",
        "    class_acc = (predictions[mask] == true_labels[mask]).mean()\n",
        "    sentiment = \"Negative\" if i == 0 else \"Neutral\" if i == 1 else \"Positive\"\n",
        "    print(f\"\\n{sentiment} Class Accuracy: {class_acc:.4f}\")\n",
        "\n",
        "# Calculate overall accuracy\n",
        "accuracy = (predictions == true_labels).mean()\n",
        "print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Save predictions to DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Text': [tokenizer.decode(ids) for ids in test_data['input_ids']],\n",
        "    'True_Label': true_labels,\n",
        "    'Predicted_Label': predictions,\n",
        "    'Confidence': np.max(probabilities, axis=1)\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "results_df.to_csv('test_predictions.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bWabnftbF2wY",
        "outputId": "7d4c83dc-c5fb-4157-9717-622d31a1a3b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-e3148c26c12f>:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/best_model.pt'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3936    0.6379    0.4868       116\n",
            "           1     0.7816    0.7022    0.7398       581\n",
            "           2     0.4923    0.4689    0.4803       273\n",
            "\n",
            "    accuracy                         0.6289       970\n",
            "   macro avg     0.5558    0.6030    0.5690       970\n",
            "weighted avg     0.6538    0.6289    0.6365       970\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxQAAAK9CAYAAAC95yoDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZrElEQVR4nO3de3zP9f//8ft7Y2+MbTZ2iiGE5RhiySkypyIqchop0Ujm1MqZzEfO5VBSfETn6BNKckzmWCJKkZqyg0PbbGxje//+6Of9fb9D9nq17b3pdv1cXpeL9/P1er9ej/e7z8pj9+fz9bLYbDabAAAAAMAEN1cXAAAAAKDooqEAAAAAYBoNBQAAAADTaCgAAAAAmEZDAQAAAMA0GgoAAAAAptFQAAAAADCNhgIAAACAaTQUAAAAAEyjoQCA6/jpp5/Url07eXt7y2KxaO3atXl6/l9++UUWi0XLly/P0/MWZa1atVKrVq1cXQYAwCAaCgCF1okTJ/TUU0/p9ttvV4kSJeTl5aVmzZpp/vz5unTpUr5eOyIiQocPH9aLL76olStXqlGjRvl6vYLUv39/WSwWeXl5Xfd7/Omnn2SxWGSxWDRr1izD5z99+rQmTZqkgwcP5kG1AIDCrpirCwCA61m/fr0eeeQRWa1W9evXT7Vr11ZWVpZ27typ0aNH68iRI3rttdfy5dqXLl1SbGysXnjhBQ0dOjRfrlGpUiVdunRJxYsXz5fz30yxYsV08eJFffLJJ3r00Ued9q1atUolSpRQRkaGqXOfPn1akydPVuXKlVW/fv1cv+/zzz83dT0AgGvRUAAodE6ePKmePXuqUqVK2rJli4KCguz7IiMjdfz4ca1fvz7frn/mzBlJko+PT75dw2KxqESJEvl2/puxWq1q1qyZ3n777WsaitWrV6tTp0768MMPC6SWixcvqlSpUvLw8CiQ6wEA8hZTngAUOjNnzlRaWpqWLVvm1ExcVa1aNQ0fPtz++sqVK5o6daqqVq0qq9WqypUr6/nnn1dmZqbT+ypXrqzOnTtr586duvvuu1WiRAndfvvt+u9//2s/ZtKkSapUqZIkafTo0bJYLKpcubKkP6cKXf2zo0mTJslisTiNbdq0Sffee698fHxUunRp1ahRQ88//7x9/43WUGzZskXNmzeXp6enfHx81KVLF33//ffXvd7x48fVv39/+fj4yNvbWwMGDNDFixdv/MX+Ra9evfTpp58qOTnZPrZv3z799NNP6tWr1zXHnz9/XqNGjVKdOnVUunRpeXl5qUOHDvr222/tx2zbtk2NGzeWJA0YMMA+derq52zVqpVq166tAwcOqEWLFipVqpT9e/nrGoqIiAiVKFHims8fHh6usmXL6vTp07n+rACA/ENDAaDQ+eSTT3T77bfrnnvuydXxTzzxhCZMmKC77rpLc+fOVcuWLRUTE6OePXtec+zx48f18MMP6/7779fs2bNVtmxZ9e/fX0eOHJEkdevWTXPnzpUkPfbYY1q5cqXmzZtnqP4jR46oc+fOyszM1JQpUzR79mw9+OCD+uqrr/72fV988YXCw8OVlJSkSZMmKSoqSrt27VKzZs30yy+/XHP8o48+qgsXLigmJkaPPvqoli9frsmTJ+e6zm7dusliseijjz6yj61evVo1a9bUXXfddc3xP//8s9auXavOnTtrzpw5Gj16tA4fPqyWLVva/3Jfq1YtTZkyRZI0aNAgrVy5UitXrlSLFi3s5zl37pw6dOig+vXra968eWrduvV165s/f77Kly+viIgIZWdnS5JeffVVff7553r55ZcVHByc688KAMhHNgAoRFJSUmySbF26dMnV8QcPHrRJsj3xxBNO46NGjbJJsm3ZssU+VqlSJZsk244dO+xjSUlJNqvVahs5cqR97OTJkzZJtpdeesnpnBEREbZKlSpdU8PEiRNtjv86nTt3rk2S7cyZMzes++o13nzzTftY/fr1bf7+/rZz587Zx7799lubm5ubrV+/ftdc7/HHH3c650MPPWTz8/O74TUdP4enp6fNZrPZHn74YVubNm1sNpvNlp2dbQsMDLRNnjz5ut9BRkaGLTs7+5rPYbVabVOmTLGP7du375rPdlXLli1tkmxLliy57r6WLVs6jW3cuNEmyTZt2jTbzz//bCtdurSta9euN/2MAICCQ0IBoFBJTU2VJJUpUyZXx2/YsEGSFBUV5TQ+cuRISbpmrUVoaKiaN29uf12+fHnVqFFDP//8s+ma/+rq2ouPP/5YOTk5uXpPfHy8Dh48qP79+8vX19c+XrduXd1///32z+lo8ODBTq+bN2+uc+fO2b/D3OjVq5e2bdumhIQEbdmyRQkJCded7iT9ue7Cze3P/2xkZ2fr3Llz9ulcX3/9da6vabVaNWDAgFwd265dOz311FOaMmWKunXrphIlSujVV1/N9bUAAPmPhgJAoeLl5SVJunDhQq6O//XXX+Xm5qZq1ao5jQcGBsrHx0e//vqr03hISMg15yhbtqz++OMPkxVfq0ePHmrWrJmeeOIJBQQEqGfPnnrvvff+trm4WmeNGjWu2VerVi2dPXtW6enpTuN//Sxly5aVJEOfpWPHjipTpozeffddrVq1So0bN77mu7wqJydHc+fOVfXq1WW1WlWuXDmVL19ehw4dUkpKSq6vedtttxlagD1r1iz5+vrq4MGDWrBggfz9/XP9XgBA/qOhAFCoeHl5KTg4WN99952h9/11UfSNuLu7X3fcZrOZvsbV+f1XlSxZUjt27NAXX3yhvn376tChQ+rRo4fuv//+a479J/7JZ7nKarWqW7duWrFihdasWXPDdEKSpk+frqioKLVo0UJvvfWWNm7cqE2bNunOO+/MdRIj/fn9GPHNN98oKSlJknT48GFD7wUA5D8aCgCFTufOnXXixAnFxsbe9NhKlSopJydHP/30k9N4YmKikpOT7Xdsygtly5Z1uiPSVX9NQSTJzc1Nbdq00Zw5c3T06FG9+OKL2rJli7Zu3Xrdc1+t89ixY9fs++GHH1SuXDl5enr+sw9wA7169dI333yjCxcuXHch+1UffPCBWrdurWXLlqlnz55q166d2rZte813ktvmLjfS09M1YMAAhYaGatCgQZo5c6b27duXZ+cHAPxzNBQACp0xY8bI09NTTzzxhBITE6/Zf+LECc2fP1/Sn1N2JF1zJ6Y5c+ZIkjp16pRndVWtWlUpKSk6dOiQfSw+Pl5r1qxxOu78+fPXvPfqA97+eivbq4KCglS/fn2tWLHC6S/o3333nT7//HP758wPrVu31tSpU/XKK68oMDDwhse5u7tfk368//77+v33353GrjY+12u+jBo7dqzi4uK0YsUKzZkzR5UrV1ZERMQNv0cAQMHjwXYACp2qVatq9erV6tGjh2rVquX0pOxdu3bp/fffV//+/SVJ9erVU0REhF577TUlJyerZcuW2rt3r1asWKGuXbve8JakZvTs2VNjx47VQw89pGeeeUYXL17U4sWLdccddzgtSp4yZYp27NihTp06qVKlSkpKStKiRYtUoUIF3XvvvTc8/0svvaQOHTooLCxMAwcO1KVLl/Tyyy/L29tbkyZNyrPP8Vdubm4aN27cTY/r3LmzpkyZogEDBuiee+7R4cOHtWrVKt1+++1Ox1WtWlU+Pj5asmSJypQpI09PTzVp0kRVqlQxVNeWLVu0aNEiTZw40X4b2zfffFOtWrXS+PHjNXPmTEPnAwDkDxIKAIXSgw8+qEOHDunhhx/Wxx9/rMjISD333HP65ZdfNHv2bC1YsMB+7Ouvv67Jkydr3759evbZZ7VlyxZFR0frnXfeydOa/Pz8tGbNGpUqVUpjxozRihUrFBMTowceeOCa2kNCQvTGG28oMjJSCxcuVIsWLbRlyxZ5e3vf8Pxt27bVZ599Jj8/P02YMEGzZs1S06ZN9dVXXxn+y3h+eP755zVy5Eht3LhRw4cP19dff63169erYsWKTscVL15cK1askLu7uwYPHqzHHntM27dvN3StCxcu6PHHH1eDBg30wgsv2MebN2+u4cOHa/bs2dq9e3eefC4AwD9jsRlZvQcAAAAADkgoAAAAAJhGQwEAAADANBoKAAAAAKbRUAAAAAAwjYYCAAAAgGk0FAAAAABMo6EAAAAAYNot+aTs40mXXF0CUCQF+5RwdQlAkeTmZnF1CUCRU6IQ/y20ZIOhLrv2pW9ecdm1zSKhAAAAAGBaIe4NAQAAABew8Dt3I/i2AAAAAJhGQwEAAADANKY8AQAAAI4s3GjBCBIKAAAAAKaRUAAAAACOWJRtCN8WAAAAUMTNmDFDFotFzz77rH0sIyNDkZGR8vPzU+nSpdW9e3clJiY6vS8uLk6dOnVSqVKl5O/vr9GjR+vKlSuGrk1DAQAAADiyWFy3mbBv3z69+uqrqlu3rtP4iBEj9Mknn+j999/X9u3bdfr0aXXr1s2+Pzs7W506dVJWVpZ27dqlFStWaPny5ZowYYKh69NQAAAAAEVUWlqaevfuraVLl6ps2bL28ZSUFC1btkxz5szRfffdp4YNG+rNN9/Url27tHv3bknS559/rqNHj+qtt95S/fr11aFDB02dOlULFy5UVlZWrmugoQAAAAAKiczMTKWmpjptmZmZNzw+MjJSnTp1Utu2bZ3GDxw4oMuXLzuN16xZUyEhIYqNjZUkxcbGqk6dOgoICLAfEx4ertTUVB05ciTXNdNQAAAAAI4sbi7bYmJi5O3t7bTFxMRct8x33nlHX3/99XX3JyQkyMPDQz4+Pk7jAQEBSkhIsB/j2Exc3X91X25xlycAAACgkIiOjlZUVJTTmNVqvea4U6dOafjw4dq0aZNKlChRUOVdFwkFAAAA4MiFi7KtVqu8vLyctus1FAcOHFBSUpLuuusuFStWTMWKFdP27du1YMECFStWTAEBAcrKylJycrLT+xITExUYGChJCgwMvOauT1dfXz0mN2goAAAAgCKmTZs2Onz4sA4ePGjfGjVqpN69e9v/XLx4cW3evNn+nmPHjikuLk5hYWGSpLCwMB0+fFhJSUn2YzZt2iQvLy+FhobmuhamPAEAAABFTJkyZVS7dm2nMU9PT/n5+dnHBw4cqKioKPn6+srLy0vDhg1TWFiYmjZtKklq166dQkND1bdvX82cOVMJCQkaN26cIiMjr5uK3AgNBQAAAODoFnlS9ty5c+Xm5qbu3bsrMzNT4eHhWrRokX2/u7u71q1bpyFDhigsLEyenp6KiIjQlClTDF3HYrPZbHldvKsdT7rk6hKAIinYx7WLuoCiys3N3MOogH+zEoX419olm4512bUv7f6Py65tViH+RwkAAAC4gMknVv9b3Rp5DgAAAACXIKEAAAAAHN0iaygKCt8WAAAAANNoKAAAAACYxpQnAAAAwBGLsg0hoQAAAABgGgkFAAAA4IhF2YbwbQEAAAAwjYYCAAAAgGlMeQIAAAAcsSjbEBIKAAAAAKaRUAAAAACOWJRtCN8WAAAAANNIKAAAAABHJBSG8G0BAAAAMI2GAgAAAIBpTHkCAAAAHLlx21gjSCgAAAAAmEZCAQAAADhiUbYhfFsAAAAATKOhAAAAAGAaU54AAAAARxYWZRtBQgEAAADANBIKAAAAwBGLsg3h2wIAAABgGgkFAAAA4Ig1FIaQUAAAAAAwjYYCAAAAgGlMeQIAAAAcsSjbEL4tAAAAAKaRUAAAAACOWJRtCAkFAAAAANNoKAAAAACYxpQnAAAAwBGLsg3h2wIAAABgGgkFAAAA4IhF2YaQUAAAAAAwjYQCAAAAcMQaCkP4tgAAAACYRkMBAAAAwDSmPAEAAACOWJRtCAkFAAAAANNIKAAAAABHLMo2hG8LAAAAgGk0FAAAAABMY8oTAAAA4IgpT4bwbQEAAAAwjYQCAAAAcMRtYw0hoQAAAABgGg0FAAAAANOY8gQAAAA4YlG2IXxbAAAAAEwjoQAAAAAcsSjbEBIKAAAAAKaRUAAAAACOWENhCN8WAAAAANNoKAAAAACYxpQnAAAAwBGLsg0hoQAAAABgGgkFAAAA4MBCQmEICQUAAAAA02goAAAAAJjGlCcAAADAAVOejCGhAAAAAGAaCQUAAADgiIDCEBIKAAAAAKaRUAAAAAAOWENhDAkFAAAAUAQtXrxYdevWlZeXl7y8vBQWFqZPP/3Uvr9Vq1ayWCxO2+DBg53OERcXp06dOqlUqVLy9/fX6NGjdeXKFUN1kFAAAAAARVCFChU0Y8YMVa9eXTabTStWrFCXLl30zTff6M4775QkPfnkk5oyZYr9PaVKlbL/OTs7W506dVJgYKB27dql+Ph49evXT8WLF9f06dNzXQcNBQAAAOCgqEx5euCBB5xev/jii1q8eLF2795tbyhKlSqlwMDA677/888/19GjR/XFF18oICBA9evX19SpUzV27FhNmjRJHh4euaqDKU8AAABAIZGZmanU1FSnLTMz86bvy87O1jvvvKP09HSFhYXZx1etWqVy5cqpdu3aio6O1sWLF+37YmNjVadOHQUEBNjHwsPDlZqaqiNHjuS6ZhoKAAAAwMFf1x0U5BYTEyNvb2+nLSYm5oa1Hj58WKVLl5bVatXgwYO1Zs0ahYaGSpJ69eqlt956S1u3blV0dLRWrlypPn362N+bkJDg1ExIsr9OSEjI9ffFlCcAAACgkIiOjlZUVJTTmNVqveHxNWrU0MGDB5WSkqIPPvhAERER2r59u0JDQzVo0CD7cXXq1FFQUJDatGmjEydOqGrVqnlWMw0FAAAAUEhYrda/bSD+ysPDQ9WqVZMkNWzYUPv27dP8+fP16quvXnNskyZNJEnHjx9X1apVFRgYqL179zodk5iYKEk3XHdxPUx5AgAAABy4csrTP5WTk3PDNRcHDx6UJAUFBUmSwsLCdPjwYSUlJdmP2bRpk7y8vOzTpnKDhAIAAAAogqKjo9WhQweFhITowoULWr16tbZt26aNGzfqxIkTWr16tTp27Cg/Pz8dOnRII0aMUIsWLVS3bl1JUrt27RQaGqq+fftq5syZSkhI0Lhx4xQZGWkoJaGhAAAAABwVjbvGKikpSf369VN8fLy8vb1Vt25dbdy4Uffff79OnTqlL774QvPmzVN6eroqVqyo7t27a9y4cfb3u7u7a926dRoyZIjCwsLk6empiIgIp+dW5IbFZrPZ8vrDudrxpEuuLuFfbcAjHZSUEH/NeKeHHtXTUc/bX9tsNk0cPVQH9nylcS/OUViL+wqyTFxHsE8JV5fwr7bs9Ve15YtN+uXkz7KWKKF69Rpo+IiRqlzl9muOtdlsGjpkkHZ99aXmzHtFrdu0dUHFuMrNrYj87eMWdWD/Pi1/Y5m+P/qdzpw5o7kLFuo+h5+Ji+npmjd3trZu+UIpycm67bYKeqxPXz3a4zEXVo0ShfjX2t69Vrrs2imr+7rs2mYV4n+UKKrmvbZK2Tk59te/njyucSMG697W9zsdt/a9t1REnhsDFIiv9+9Tj569dGftOrqSna1X5s/VkKee0Edr16mkw5NNJWnVyhVF5sFLQH67dOmiatSooa7duitq+NBr9s+aOUN79+zW9BkvKfi22xT71VeaPm2y/Mv7q9V9bVxQMQo7/v1qDA0F8px3WV+n1x+sekNBt1VUnfqN7GMnfvpBa95dqXlLV6tvV36zCkjSwiWvO72ePC1GbVreo6NHj6hho8b28WM/fK+VK97Uqnc/0P2tmxd0mUChc2/zlrq3ecsb7j948Bs90KWrGt/95x1uHn60hz54/119d/gQDQWQB1zaUJw9e1ZvvPGGYmNj7Q/PCAwM1D333KP+/furfPnyriwPeeDy5cva+vkGdX20j73bz8i4pJcmP68hI6Ll61fOxRUChVda2gVJkre3t33s0qVLih47Ss+9MEHlyvHvSCA36tdvoO1bt6hrt4fl7++vfXv36NdfTmr02GhXlwbcElzWUOzbt0/h4eEqVaqU2rZtqzvuuEPSn/e+XbBggWbMmKGNGzeqUaNGf3uezMzMa26NlZmZY2hlOvLP7i+3KC3tgtp2fNA+tvTlWapVu57Cmrd2YWVA4ZaTk6NZ/5mu+g3uUrXqd9jHZ8+MUb36DdSa36oCufbcC+M1ZeJ4tbuvhYoVKyaLxaKJk6c5JX+AI6Y8GeOyhmLYsGF65JFHtGTJkmv+odlsNg0ePFjDhg1TbGzs354nJiZGkydPdj73qOf1zOhxN3gHCtLn69aqUZNm8ivnL0navXObDn29VwuWveviyoDCLebFKTp+/Ce9uWK1fWzb1i3au3eP3nn/IxdWBhQ9b69aqUOHDmr+K4sVHBysA/v3a/q0ySrv76+mYfe4ujygyHNZQ/Htt99q+fLl1+0ALRaLRowYoQYNGtz0PNd7PPmplJwbHI2ClJRwWgcP7NHz02bbxw59vVfxv/+mRzs6z/uePn6U7qzbQDNeXlbQZQKFzowXp+jL7du0bPlbCnB4Uum+vbv126k4tbjnbqfjR0U9owZ3NdTrb7ruriRAYZWRkaEF8+Zq7oJX1KJlK0nSHTVq6tix77XizWU0FLguEgpjXNZQXH3Ud82aNa+7f+/evQoICLjpea73eHJrBreNLQw2bfhY3j6+ujvs/5qHh3s/rnaduzkdFxnxsJ4cNkp333PjBXXAv4HNZtN/pk/Vli1faOkb/9VtFSo47R8w8Ek91O1hp7FHuj2okWOeU8uW3HYZuJ4rV67oypXL19za183NXTm33p3zAZdwWUMxatQoDRo0SAcOHFCbNm3szUNiYqI2b96spUuXatasWa4qD/9QTk6ONm34n9p0eEDuxf7v/2a+fuWuuxC7vH+gAoNvK8gSgUIn5sUp+nTDOs2dv1Cenp46e/aMJKl06TIqUaKEypUrf92F2EGBwdc0H8C/ycX0dMXFxdlf//7bb/rh++/l7e2toOBgNWp8t+bMeklWawkFBQfrwL59Wve/tRo15jkXVg3cOlzWUERGRqpcuXKaO3euFi1apOzsbEl/PrGvYcOGWr58uR599FFXlYd/6OD+3TqTGK92Hbu6uhSgyHj/3bclSU8+3s9pfPLU6Xqwa7frvQWApCNHvtMTA/7v52bWzBhJ0oNdHtLU6TP0n5fmaP68OYoeO0qpKSkKCg7W0GdG6BEebIcbYMqTMYXiSdmXL1/W2bNnJUnlypVT8eLF/9H5eFI2YA5PygbM4UnZgHGF+UnZfv3edtm1z/236DW6heIfZfHixRUUFOTqMgAAAACJ3xEY4ubqAgAAAAAUXYUioQAAAAAKC9ZQGENCAQAAAMA0GgoAAAAApjHlCQAAAHDAlCdjSCgAAAAAmEZCAQAAADggoTCGhAIAAACAaTQUAAAAAExjyhMAAADgiBlPhpBQAAAAADCNhAIAAABwwKJsY0goAAAAAJhGQgEAAAA4IKEwhoQCAAAAgGk0FAAAAABMY8oTAAAA4IApT8aQUAAAAAAwjYQCAAAAcEBCYQwJBQAAAADTaCgAAAAAmMaUJwAAAMARM54MIaEAAAAAYBoJBQAAAOCARdnGkFAAAAAAMI2EAgAAAHBAQmEMCQUAAAAA02goAAAAAJjGlCcAAADAAVOejCGhAAAAAGAaCQUAAADgiIDCEBIKAAAAAKbRUAAAAAAwjSlPAAAAgAMWZRtDQgEAAADANBIKAAAAwAEJhTEkFAAAAABMo6EAAAAAYBpTngAAAAAHTHkyhoQCAAAAgGkkFAAAAIADEgpjSCgAAAAAmEZCAQAAADgioDCEhAIAAACAaTQUAAAAAExjyhMAAADggEXZxpBQAAAAADCNhAIAAABwQEJhDAkFAAAAANNoKAAAAACYxpQnAAAAwAEznowhoQAAAABgGgkFAAAA4IBF2caQUAAAAAAwjYQCAAAAcEBAYQwJBQAAAADTaCgAAAAAmEZDAQAAADiwWCwu24xYvHix6tatKy8vL3l5eSksLEyffvqpfX9GRoYiIyPl5+en0qVLq3v37kpMTHQ6R1xcnDp16qRSpUrJ399fo0eP1pUrVwzVQUMBAAAAFEEVKlTQjBkzdODAAe3fv1/33XefunTpoiNHjkiSRowYoU8++UTvv/++tm/frtOnT6tbt27292dnZ6tTp07KysrSrl27tGLFCi1fvlwTJkwwVIfFZrPZ8vSTFQLHky65ugSgSAr2KeHqEoAiyc2NFZyAUSUK8a2Baj630WXX/mFG+D96v6+vr1566SU9/PDDKl++vFavXq2HH374z3P/8INq1aql2NhYNW3aVJ9++qk6d+6s06dPKyAgQJK0ZMkSjR07VmfOnJGHh0eurklCAQAAABQSmZmZSk1NddoyMzNv+r7s7Gy98847Sk9PV1hYmA4cOKDLly+rbdu29mNq1qypkJAQxcbGSpJiY2NVp04dezMhSeHh4UpNTbWnHLlBQwEAAAAUEjExMfL29nbaYmJibnj84cOHVbp0aVmtVg0ePFhr1qxRaGioEhIS5OHhIR8fH6fjAwIClJCQIElKSEhwaiau7r+6L7cKcdgEAAAAFDxXTmOMjo5WVFSU05jVar3h8TVq1NDBgweVkpKiDz74QBEREdq+fXt+l+mEhgIAAAAoJKxW6982EH/l4eGhatWqSZIaNmyoffv2af78+erRo4eysrKUnJzslFIkJiYqMDBQkhQYGKi9e/c6ne/qXaCuHpMbTHkCAAAAHFgsrtv+qZycHGVmZqphw4YqXry4Nm/ebN937NgxxcXFKSwsTJIUFhamw4cPKykpyX7Mpk2b5OXlpdDQ0Fxfk4QCAAAAKIKio6PVoUMHhYSE6MKFC1q9erW2bdumjRs3ytvbWwMHDlRUVJR8fX3l5eWlYcOGKSwsTE2bNpUktWvXTqGhoerbt69mzpyphIQEjRs3TpGRkYZSEhoKAAAAwIHRB8y5SlJSkvr166f4+Hh5e3urbt262rhxo+6//35J0ty5c+Xm5qbu3bsrMzNT4eHhWrRokf397u7uWrdunYYMGaKwsDB5enoqIiJCU6ZMMVQHz6EAYMdzKABzeA4FYFxhfg5F7XGbXHbt76bd77Jrm8UaCgAAAACmFeLeEAAAACh4RWTGU6FBQgEAAADANBIKAAAAwEFRWZRdWJBQAAAAADCNhgIAAACAaUx5AgAAABww5ckYEgoAAAAAppFQAAAAAA4IKIwhoQAAAABgGgkFAAAA4IA1FMaQUAAAAAAwjYYCAAAAgGlMeQIAAAAcMOPJGBIKAAAAAKaRUAAAAAAOWJRtDAkFAAAAANNoKAAAAACYxpQnAAAAwAEznowhoQAAAABgGgkFAAAA4IBF2caQUAAAAAAwjYQCAAAAcEBAYQwJBQAAAADTaCgAAAAAmMaUJwAAAMABi7KNIaEAAAAAYBoJBQAAAOCAgMKYW7KhKFGc4AUww6/JMFeXABRJW9+f5uoSgCKnaTUfV5eAPMLfvAEAAACYdksmFAAAAIBZLMo2hoQCAAAAgGkkFAAAAIADAgpjSCgAAAAAmEZCAQAAADhgDYUxJBQAAAAATKOhAAAAAGAaU54AAAAAB8x4MoaEAgAAAIBpJBQAAACAAxZlG0NCAQAAAMA0GgoAAAAApjHlCQAAAHDAlCdjSCgAAAAAmEZCAQAAADggoDCGhAIAAACAaTQUAAAAAExjyhMAAADggEXZxpBQAAAAADCNhAIAAABwQEBhDAkFAAAAANNIKAAAAAAHrKEwhoQCAAAAgGk0FAAAAABMY8oTAAAA4IAZT8aQUAAAAAAwjYQCAAAAcOBGRGEICQUAAAAA02goAAAAAJjGlCcAAADAATOejCGhAAAAAGAaCQUAAADggCdlG0NCAQAAAMA0EgoAAADAgRsBhSEkFAAAAABMo6EAAAAAYBpTngAAAAAHLMo2hoQCAAAAgGk0FAAAAIADi8V1mxExMTFq3LixypQpI39/f3Xt2lXHjh1zOqZVq1ayWCxO2+DBg52OiYuLU6dOnVSqVCn5+/tr9OjRunLlSq7rYMoTAAAAUARt375dkZGRaty4sa5cuaLnn39e7dq109GjR+Xp6Wk/7sknn9SUKVPsr0uVKmX/c3Z2tjp16qTAwEDt2rVL8fHx6tevn4oXL67p06fnqg4aCgAAAKAI+uyzz5xeL1++XP7+/jpw4IBatGhhHy9VqpQCAwOve47PP/9cR48e1RdffKGAgADVr19fU6dO1dixYzVp0iR5eHjctA6mPAEAAAAOLC78X2ZmplJTU522zMzMXNWdkpIiSfL19XUaX7VqlcqVK6fatWsrOjpaFy9etO+LjY1VnTp1FBAQYB8LDw9Xamqqjhw5kqvr0lAAAAAAhURMTIy8vb2dtpiYmJu+LycnR88++6yaNWum2rVr28d79eqlt956S1u3blV0dLRWrlypPn362PcnJCQ4NROS7K8TEhJyVTNTngAAAAAHrnxSdnR0tKKiopzGrFbrTd8XGRmp7777Tjt37nQaHzRokP3PderUUVBQkNq0aaMTJ06oatWqeVIzCQUAAABQSFitVnl5eTltN2sohg4dqnXr1mnr1q2qUKHC3x7bpEkTSdLx48clSYGBgUpMTHQ65urrG627+CsaCgAAAMDBX2+zWpCbETabTUOHDtWaNWu0ZcsWValS5abvOXjwoCQpKChIkhQWFqbDhw8rKSnJfsymTZvk5eWl0NDQXNXBlCcAAACgCIqMjNTq1av18ccfq0yZMvY1D97e3ipZsqROnDih1atXq2PHjvLz89OhQ4c0YsQItWjRQnXr1pUktWvXTqGhoerbt69mzpyphIQEjRs3TpGRkbmaaiWRUAAAAABF0uLFi5WSkqJWrVopKCjIvr377ruSJA8PD33xxRdq166datasqZEjR6p79+765JNP7Odwd3fXunXr5O7urrCwMPXp00f9+vVzem7FzZBQAAAAAA6MPrHaVWw229/ur1ixorZv337T81SqVEkbNmwwXQcJBQAAAADTSCgAAAAAB25FJaIoJEgoAAAAAJhGQwEAAADANKY8AQAAAA6Y8WQMCQUAAAAA00goAAAAAAdGn1j9b0dCAQAAAMA0EgoAAADAAQGFMSQUAAAAAEyjoQAAAABgGlOeAAAAAAc8KdsYEgoAAAAAppFQAAAAAA7IJ4whoQAAAABgGg0FAAAAANOY8gQAAAA44EnZxpBQAAAAADAtVwnFoUOHcn3CunXrmi4GAAAAcDU3AgpDctVQ1K9fXxaLRTab7br7r+6zWCzKzs7O0wIBAAAAFF65aihOnjyZ33UAAAAAhQJrKIzJVUNRqVKl/K4DAAAAQBFkalH2ypUr1axZMwUHB+vXX3+VJM2bN08ff/xxnhYHAAAAoHAz3FAsXrxYUVFR6tixo5KTk+1rJnx8fDRv3ry8rg8AAAAoUBaL67aiyHBD8fLLL2vp0qV64YUX5O7ubh9v1KiRDh8+nKfFAQAAACjcDD/Y7uTJk2rQoME141arVenp6XlSFAAAAOAqLMo2xnBCUaVKFR08ePCa8c8++0y1atXKi5oAAAAAFBGGE4qoqChFRkYqIyNDNptNe/fu1dtvv62YmBi9/vrr+VEjAAAAgELKcEPxxBNPqGTJkho3bpwuXryoXr16KTg4WPPnz1fPnj3zo0YAAACgwPCkbGMMNxSS1Lt3b/Xu3VsXL15UWlqa/P3987ouAAAAAEWAqYZCkpKSknTs2DFJfy5cKV++fJ4VBQAAALgKi7KNMbwo+8KFC+rbt6+Cg4PVsmVLtWzZUsHBwerTp49SUlLyo0YAAAAAhZThhuKJJ57Qnj17tH79eiUnJys5OVnr1q3T/v379dRTT+VHjQAAAECBsbhwK4oMT3lat26dNm7cqHvvvdc+Fh4erqVLl6p9+/Z5WhwAAACAws1wQuHn5ydvb+9rxr29vVW2bNk8KQoAAABA0WC4oRg3bpyioqKUkJBgH0tISNDo0aM1fvz4PC0OAAAAKGhuFovLtqIoV1OeGjRo4LTa/aefflJISIhCQkIkSXFxcbJarTpz5gzrKAAAAIB/kVw1FF27ds3nMgAAAIDCoYgGBS6Tq4Zi4sSJ+V0HAAAAgCLI8BoKAAAAALjK8G1js7OzNXfuXL333nuKi4tTVlaW0/7z58/nWXEAAABAQeNJ2cYYTigmT56sOXPmqEePHkpJSVFUVJS6desmNzc3TZo0KR9KBAAAAFBYGW4oVq1apaVLl2rkyJEqVqyYHnvsMb3++uuaMGGCdu/enR81AgAAAAXGYnHdVhQZbigSEhJUp04dSVLp0qWVkpIiSercubPWr1+ft9UBAAAAKNQMNxQVKlRQfHy8JKlq1ar6/PPPJUn79u2T1WrN2+oAAAAAFGqGF2U/9NBD2rx5s5o0aaJhw4apT58+WrZsmeLi4jRixIj8qBEAAAAoMEX1idWuYrihmDFjhv3PPXr0UKVKlbRr1y5Vr15dDzzwQJ4Wh6JpxdJF+u+yJU5jFStV1vJ3/ydJOv3bKS15eba++/YbXc7KUuOwZhoaFS1fPz9XlAsUCqMG3K+pz3TRK6u2avSsDyVJVo9imhHVTY+EN5TVo5i+iP1ew6e/q6TzF+zvaxgaoqnPdFGD0Iqy2aT93/2qF+av1eEff3fVRwHy1Q/ffaNPP3xLvxz/Qcnnz+qZcTPVMKylff/+r7Zqy6cf6ZfjPyj9QqqmLFipSlXvuO65bDabZk8cocMHYq85D4Dc+8fPoWjatKmioqLUpEkTTZ8+PS9qwi2g8u1V9f76LfZt/qsrJEmXLl3UmOFPySKLZr2yVPNfW6HLly9r3OhhysnJcXHVgGs0DA3RwO7NdOjH35zGZ47qrk4taqv3mGVq98Q8BZX31juzn7Dv9yzpoY8XRupUwh9q0XeW2gyYo7SLGfrfwkgVK8ZjhnBrysy4pIpVqqvvkNHX3595SXeE1tOjA4be9Fwb175TZBfBIn+xKNuYPPsvTnx8vMaPH59Xp0MR5+5eTL5+5eybt09ZSdKRQweVGH9aYyZM1e3V7tDt1e7Q2AnT9OP3R/TN/r0urhooeJ4lPfTm9P56eurbSk69ZB/3Kl1C/buGaeycj7R934/65vtTGjTxLYXVr6q761SWJNWoEig/H09NXbxOP/2apO9/TtCLr36qwHJeCgnyddEnAvJXvUb36OF+g9XonlbX3d/svo7q2usJ3Vm/8d+e59cTP+qzNas0cDh/dwH+KX6FhXzx+6lf9WjnNurTrYOmT3hOiQl/LuTPysqSLBYVL+5hP9bDwyqLm5u++/ZrV5ULuMy86B767MvvtHXPMafxBrVC5FG8mLbs/r/xH39JVFz8eTWpW8X++uwfaYroeo+KF3NXCWtx9e8apu9/jtevp3nIKHAjmRkZWvLSePUbMlo+vky3xbUsFovLtqKIhgJ5ruaddTRm/DTFzF2s4WPGKT7+dz07uL8upqcrtHZdlSxRUksXzlVGxiVdunRRry6YrZzsbJ07d9bVpQMF6pHwhqpfs6LGv/y/a/YF+nkpM+uyUtIuOY0nnUtVgJ+XJCntYqbCn5yvxzo21h+75+rsV7N1/z211HXoImVnM4UQuJHVS+eqWq26uos1E0CeKNQNxalTp/T444//7TGZmZlKTU112jIzMwuoQlxPk3uaq2Wbdqpa/Q41btpMMXMWKv3CBW3bvFE+ZX01Yfosxe7crs6tm+rBts2UlnZB1WvU4o4K+FepEOCjl0Z314AXlisz64qpc5SwFteSib0V++3Patlvlu4bMEdHT8TrowVDVMJaPI8rBm4NX+/eoe8P7VfvQdyZEsgrub7LU1RU1N/uP3PmzD8u5q/Onz+vFStW6I033rjhMTExMZo8ebLT2IgxLyjqOeZEFhaly3ipQkglnf7tlCSpUZN79NaHG5SS/Ifc3d1VuoyXHu7YWkG3VXBxpUDBaVArRAF+XopdPdY+VqyYu+69q6oG92ihByIXyupRXN6lSzqlFP5+Xko8lypJ6tGhkUKCfdUyYrZsNpskKSJ6ueJ3zNQDrerq/Y0HCvZDAUXA94f2Kyn+dw15tK3T+MvTn1ONO+sresZiF1WGwqRQ/8a9EMp1Q/HNN9/c9JgWLVoYuvj//ndtzO/o559/vuk5oqOjr2l2zlw0VAby2aWLF3X691Nq276z0/jVhdrf7N+j5D/O657mrVxQHeAaW/ceU8OHX3Qae21yHx07majZyzfpt8Q/lHX5ilo3qaG1mw9KkqpX8ldIkK/2HDopSSpVwkM5OTZ7MyFJOTabbDbuoQ7cSKeHI9SyXRensRcie6nXk8+qwd3NXVQVULTluqHYunVrnl+8a9euslgsTv8x/KubLU6xWq3XPKE7NZspT660ZMEshd3bSgGBQTp39oyWL10kNzd33deugyTps3VrFVK5inx8fHXk8LdaOPc/6t6zrypWquLiyoGCk3YxU0dPxDuNpV/K0vmUdPv48rWx+s/Ibjqfkq4L6RmaM/YR7f72Z+09/IskafPuHzT92a6aF/2oFr+zXW4Wi0YNaKcr2dnavv/Hgv5IQIHIuHRRiaf/7xbLZxJO69cTP6p0GS/5+Qcq7UKKziUlKvn8nzMnEn7/VZLkXdZPPr7/t/2VX/lAlQ8MLpgPgUKvqC6OdhXDD7bLS0FBQVq0aJG6dOly3f0HDx5Uw4YNC7gq/FNnkpL04oSxSk1JlrdPWdWud5deef0t+ZT98zaWp379Ra8vmq8LqSkKCLpNvfs/qYcf6+viqoHCZ8ysD5WTY9Pbs57488F2u77X8Jh37ft//CVR3Ye/qhee6qBtK0YqJ8emb3/4TV0iFynhbKoLKwfyz8mfvteM6Kftr99+fZ4k6d42nfRk1AR9s/tLvT5vqn3/ov+MkyR17fWEHur9ZIHWCvxbWGx/Fw/kswcffFD169fXlClTrrv/22+/VYMGDQw/8Oy3P0goADOq3zfS1SUARdLW96e5ugSgyGlazcfVJdzQM2t/cNm1F3St6bJrm+XShGL06NFKT0+/4f5q1arly1QrAAAA4EbcmPFkiEsbiubN/37xk6enp1q25B7RAAAAQGHl0oYCAAAAKGxIKIwxdZvdL7/8Un369FFYWJh+//13SdLKlSu1c+fOPC0OAAAAQOFmuKH48MMPFR4erpIlS+qbb76xP5U6JSVF06dPz/MCAQAAgIJksVhcthVFhhuKadOmacmSJVq6dKmKFy9uH2/WrJm+/vrrPC0OAAAAQOFmuKE4duzYdZ+I7e3treTk5LyoCQAAAEARYbihCAwM1PHjx68Z37lzp26//fY8KQoAAABwFTeL67aiyHBD8eSTT2r48OHas2ePLBaLTp8+rVWrVmnUqFEaMmRIftQIAAAAoJAyfNvY5557Tjk5OWrTpo0uXryoFi1ayGq1atSoURo2bFh+1AgAAAAUmCK6NtplDDcUFotFL7zwgkaPHq3jx48rLS1NoaGhKl26dH7UBwAAAKAQM/UcCkny8PBQaGio7r77bpoJAAAAoIDFxMSocePGKlOmjPz9/dW1a1cdO3bM6ZiMjAxFRkbKz89PpUuXVvfu3ZWYmOh0TFxcnDp16qRSpUrJ399fo0eP1pUrV3Jdh+GEonXr1n97j9wtW7YYPSUAAABQaLgVkTlP27dvV2RkpBo3bqwrV67o+eefV7t27XT06FF5enpKkkaMGKH169fr/fffl7e3t4YOHapu3brpq6++kiRlZ2erU6dOCgwM1K5duxQfH69+/fqpePHiuX7GnOGGon79+k6vL1++rIMHD+q7775TRESE0dMBAAAAMOGzzz5zer18+XL5+/vrwIEDatGihVJSUrRs2TKtXr1a9913nyTpzTffVK1atbR79241bdpUn3/+uY4ePaovvvhCAQEBql+/vqZOnaqxY8dq0qRJ8vDwuGkdhhuKuXPnXnd80qRJSktLM3o6AAAAoFAxvSYgD2RmZiozM9NpzGq1ymq13vS9KSkpkiRfX19J0oEDB3T58mW1bdvWfkzNmjUVEhKi2NhYNW3aVLGxsapTp44CAgLsx4SHh2vIkCE6cuSIGjRocNPr5tn31adPH73xxht5dToAAADgXycmJkbe3t5OW0xMzE3fl5OTo2effVbNmjVT7dq1JUkJCQny8PCQj4+P07EBAQFKSEiwH+PYTFzdf3VfbhhOKG4kNjZWJUqUyKvTAQAAAC7hyiUU0dHRioqKchrLTToRGRmp7777Tjt37syv0m7IcEPRrVs3p9c2m03x8fHav3+/xo8fn2eFAQAAAP82uZ3e5Gjo0KFat26dduzYoQoVKtjHAwMDlZWVpeTkZKeUIjExUYGBgfZj9u7d63S+q3eBunrMzRie8vTXCMbX11etWrXShg0bNHHiRKOnAwAAAGCCzWbT0KFDtWbNGm3ZskVVqlRx2t+wYUMVL15cmzdvto8dO3ZMcXFxCgsLkySFhYXp8OHDSkpKsh+zadMmeXl5KTQ0NFd1GEoosrOzNWDAANWpU0dly5Y18lYAAACgSCgqt42NjIzU6tWr9fHHH6tMmTL2NQ/e3t4qWbKkvL29NXDgQEVFRcnX11deXl4aNmyYwsLC1LRpU0lSu3btFBoaqr59+2rmzJlKSEjQuHHjFBkZmeukxFBC4e7urnbt2ik5OdnYpwUAAACQpxYvXqyUlBS1atVKQUFB9u3dd9+1HzN37lx17txZ3bt3V4sWLRQYGKiPPvrIvt/d3V3r1q2Tu7u7wsLC1KdPH/Xr109TpkzJdR2G11DUrl1bP//88zWRCgAAAHArKCIBhWw2202PKVGihBYuXKiFCxfe8JhKlSppw4YNpuswvIZi2rRpGjVqlNatW6f4+HilpqY6bQAAAAD+PXKdUEyZMkUjR45Ux44dJUkPPvigLA7tm81mk8ViUXZ2dt5XCQAAAKBQynVDMXnyZA0ePFhbt27Nz3oAAAAAl3IrIlOeCotcNxRX52i1bNky34oBAAAAULQYWpRtKSorVAAAAACTisptYwsLQw3FHXfccdOm4vz58/+oIAAAAABFh6GGYvLkyfL29s6vWgAAAACXI6AwxlBD0bNnT/n7++dXLQAAAACKmFw/h4L1EwAAAAD+yvBdngAAAIBbGbeNNSbXDUVOTk5+1gEAAACgCDK0hgIAAAC41VlERGFErtdQAAAAAMBf0VAAAAAAMI0pTwAAAIADFmUbQ0IBAAAAwDQSCgAAAMABCYUxJBQAAAAATCOhAAAAABxYLEQURpBQAAAAADCNhgIAAACAaUx5AgAAABywKNsYEgoAAAAAppFQAAAAAA5Yk20MCQUAAAAA02goAAAAAJjGlCcAAADAgRtzngwhoQAAAABgGgkFAAAA4IDbxhpDQgEAAADANBIKAAAAwAFLKIwhoQAAAABgGg0FAAAAANOY8gQAAAA4cBNznowgoQAAAABgGgkFAAAA4IBF2caQUAAAAAAwjYYCAAAAgGlMeQIAAAAc8KRsY0goAAAAAJhGQgEAAAA4cGNVtiEkFAAAAABMo6EAAAAAYBpTngAAAAAHzHgyhoQCAAAAgGkkFAAAAIADFmUbQ0IBAAAAwDQSCgAAAMABAYUxJBQAAAAATKOhAAAAAGAaU54AAAAAB/zG3Ri+LwAAAACmkVAAAAAADiysyjaEhAIAAACAaTQUAAAAAExjyhMAAADggAlPxpBQAAAAADCNhAIAAABw4MaibENIKAAAAACYRkIBAAAAOCCfMIaEAgAAAIBpNBQAAAAATGPKEwAAAOCANdnGkFAAAAAAMI2EAgAAAHBgIaIwhIQCAAAAgGk0FAAAAABMY8oTAAAA4IDfuBvD9wUAAAAUQTt27NADDzyg4OBgWSwWrV271ml///79ZbFYnLb27ds7HXP+/Hn17t1bXl5e8vHx0cCBA5WWlmaoDhoKAAAAwMFf/xJekJsR6enpqlevnhYuXHjDY9q3b6/4+Hj79vbbbzvt7927t44cOaJNmzZp3bp12rFjhwYNGmSoDqY8AQAAAEVQhw4d1KFDh789xmq1KjAw8Lr7vv/+e3322Wfat2+fGjVqJEl6+eWX1bFjR82aNUvBwcG5qoOEAgAAAHBgceGWmZmp1NRUpy0zM9P0Z9m2bZv8/f1Vo0YNDRkyROfOnbPvi42NlY+Pj72ZkKS2bdvKzc1Ne/bsyfU1aCgAAACAQiImJkbe3t5OW0xMjKlztW/fXv/973+1efNm/ec//9H27dvVoUMHZWdnS5ISEhLk7+/v9J5ixYrJ19dXCQkJub4OU54AAACAQiI6OlpRUVFOY1ar1dS5evbsaf9znTp1VLduXVWtWlXbtm1TmzZt/lGdjmgoAAAAAAeufFK21Wo13UDczO23365y5crp+PHjatOmjQIDA5WUlOR0zJUrV3T+/Pkbrru4nluyoeBx6YA5+z6Z4eoSgCIpMS3D1SUAwE399ttvOnfunIKCgiRJYWFhSk5O1oEDB9SwYUNJ0pYtW5STk6MmTZrk+ry3ZEMBAAAAmFVUFhmnpaXp+PHj9tcnT57UwYMH5evrK19fX02ePFndu3dXYGCgTpw4oTFjxqhatWoKDw+XJNWqVUvt27fXk08+qSVLlujy5csaOnSoevbsmes7PElF5/sCAAAA4GD//v1q0KCBGjRoIEmKiopSgwYNNGHCBLm7u+vQoUN68MEHdccdd2jgwIFq2LChvvzyS6cpVatWrVLNmjXVpk0bdezYUffee69ee+01Q3VYbDabLU8/WSHwe3KWq0sAiqQ/0vjZAcxgyhNgXJua5Vxdwg199G28y67drV6Qy65tFlOeAAAAAAesxzWGKU8AAAAATCOhAAAAAByQTxhDQgEAAADANBIKAAAAwAFLKIwhoQAAAABgGg0FAAAAANOY8gQAAAA4cGNZtiEkFAAAAABMI6EAAAAAHLAo2xgSCgAAAACm0VAAAAAAMI0pTwAAAIADC4uyDSGhAAAAAGAaCQUAAADggEXZxpBQAAAAADCNhAIAAABwwIPtjCGhAAAAAGAaDQUAAAAA05jyBAAAADhgUbYxJBQAAAAATCOhAAAAAByQUBhDQgEAAADANBoKAAAAAKYx5QkAAABwYOE5FIaQUAAAAAAwjYQCAAAAcOBGQGEICQUAAAAA00goAAAAAAesoTCGhAIAAACAaTQUAAAAAExjyhMAAADggCdlG0NCAQAAAMA0EgoAAADAAYuyjSGhAAAAAGAaDQUAAAAA05jyBAAAADjgSdnGkFAAAAAAMI2EAgAAAHDAomxjSCgAAAAAmEZDAQAAAMA0pjwBAAAADnhStjEkFAAAAABMI6EAAAAAHBBQGENCAQAAAMA0EgoAAADAgRuLKAwhoQAAAABgGg0FAAAAANOY8gQAAAA4YMKTMSQUAAAAAEwjoQAAAAAcEVEYQkIBAAAAwDQaCgAAAACmMeUJAAAAcGBhzpMhJBQAAAAATCOhAAAAABzwoGxjSCgAAAAAmEZCAQAAADggoDCGhAIAAACAaTQUAAAAAExjyhMAAADgiDlPhpBQAAAAADCNhAIAAABwwIPtjCGhAAAAAGAaDQUAAAAA05jyBAAAADjgSdnGkFAAAAAAMI2EAgAAAHBAQGEMCQUAAABQBO3YsUMPPPCAgoODZbFYtHbtWqf9NptNEyZMUFBQkEqWLKm2bdvqp59+cjrm/Pnz6t27t7y8vOTj46OBAwcqLS3NUB00FAAAAIAjiws3A9LT01WvXj0tXLjwuvtnzpypBQsWaMmSJdqzZ488PT0VHh6ujIwM+zG9e/fWkSNHtGnTJq1bt047duzQoEGDDNVhsdlsNmOlF36/J2e5ugSgSPojjZ8dwIzEtIybHwTASZua5Vxdwg19/Wuqy659VyUvU++zWCxas2aNunbtKunPdCI4OFgjR47UqFGjJEkpKSkKCAjQ8uXL1bNnT33//fcKDQ3Vvn371KhRI0nSZ599po4dO+q3335TcHBwrq5NQgEAAAAUEpmZmUpNTXXaMjMzDZ/n5MmTSkhIUNu2be1j3t7eatKkiWJjYyVJsbGx8vHxsTcTktS2bVu5ublpz549ub4WDQUAAADgwOLC/8XExMjb29tpi4mJMfwZEhISJEkBAQFO4wEBAfZ9CQkJ8vf3d9pfrFgx+fr62o/JDe7yBAAAABQS0dHRioqKchqzWq0uqiZ3aCgAAAAAB658sJ3Vas2TBiIwMFCSlJiYqKCgIPt4YmKi6tevbz8mKSnJ6X1XrlzR+fPn7e/PDaY8AQAAALeYKlWqKDAwUJs3b7aPpaamas+ePQoLC5MkhYWFKTk5WQcOHLAfs2XLFuXk5KhJkya5vhYJBQAAAFAEpaWl6fjx4/bXJ0+e1MGDB+Xr66uQkBA9++yzmjZtmqpXr64qVapo/PjxCg4Ott8JqlatWmrfvr2efPJJLVmyRJcvX9bQoUPVs2fPXN/hSaKhAAAAAJwUlSdl79+/X61bt7a/vrr2IiIiQsuXL9eYMWOUnp6uQYMGKTk5Wffee68+++wzlShRwv6eVatWaejQoWrTpo3c3NzUvXt3LViwwFAdPIcCgB3PoQDM4TkUgHGF+TkU38ZdcNm164WUcdm1zSKhAAAAABwVlYiikGBRNgAAAADTSCgAAAAABxYiCkNIKAAAAACYRkMBAAAAwDSmPAEAAAAOXPmk7KKIhAIAAACAaSQUAAAAgAMCCmNIKAAAAACYRkMBAAAAwDSmPAEAAACOmPNkCAkFAAAAANNIKAAAAAAHPCnbGBIKAAAAAKaRUAAAAAAOeLCdMTQUyHPLly7Sf19f7DRWsVJlrXjvE0nSiCED9O3X+532P/DQIxrx3IQCqxEojC5dTNfbby7Wnp1blZr8h6pUq6HHI0epWs07JUk2m03vLF+iLzas0cW0NNWoXU+DhkcruEKIiysHCs5PRw5q05rVOnX8B6X8cU6DomNUv2kLSVL2lSv636rXdORArM4mnFbJUp6qUa+xuvYbLB+/8vZzJP4epzXLF+rE94eVfeWybqtcTZ17PaEadRu66mMBRRoNBfJF5duradYrS+2v3d3dnfZ36tJdA54aan9ttZYosNqAwmrR7KmKO3lCz0RPla9fee34YoMmjxmiecs+kF95f619Z4U2rHlHw8ZOln/gbXpn+WJNfW6o5r/xvjw8rK4uHygQWRmXVKFyNd3TppNem/G8877MDJ06cUwdHu2vCpWr6WL6Bb2/dL6WvDhWz815w37c4mljVD6ogoZPWyAPD6u2/O89LZ42RpNffU/eZf0K+iMBRR5rKJAv3N3d5etXzr55+5R12m8tUdJpv2fp0i6qFCgcMjMztHvHFvUb9IzurHuXgm6rqB4RTykwuKI2fvKBbDab1n20Wg/3Gai7m7VS5arVNWzsZP1x9oz27tzm6vKBAnNnwzA92GeQ6oe1vGZfSc/SembKfDW8t40CKlRSlRq19ehTUYo7cUznzyRIktJSk5V0+pTCu/dRhcrV5B9cUV37DVZWZobif/25oD8OCimLC7eiiIYC+eL3U3F6pNN96v1Qe704YawSE+Kd9m/euF5d2zXX4489pKUL5ykj45KLKgUKh5zsbOXkZKv4X5IGD6tVP3x3UInxvyv5/DnVvauJfZ9n6TKqXqu2jh09VNDlAkVGRnqaLBaLSnqWkSR5lvFWwG0h2rP1M2VmXFJ29hV9ufFjlfEuq5BqNVxcLVA0uXzK06VLl3TgwAH5+voqNDTUaV9GRobee+899evX74bvz8zMVGZm5l/GLLJaif9dpdaddTRmwlRVDKms8+fOasXrizX8qQi9sXqNSnl6qk27jgoICpZfufL6+fiPeu2VuToV94um/Geeq0sHXKZkKU/VCK2rD956XRVCqsi7rK92btmoH48eVmBwRSX/cU6S5FPW1+l93mV97fsAOLuclak1/12sRs3bqmQpT0mSxWLRM1Pm69Xpzymq5/2yWNxUxttHQyfNUanSXi6uGIVGUY0KXMSlCcWPP/6oWrVqqUWLFqpTp45atmyp+Pj/+012SkqKBgwY8LfniImJkbe3t9P2ytyZ+V06/kaTe5qrVZtwVa1eQ42bNtOMuYuUfuGCtm3eKEnq/NAjaty0mW6vdofatu+s5yZN185tm/X7b6dcXDngWs9ET5HNZtOTPdqrZ/swbVjzju5tHS6LG/9lA4zKvnJFr88cL9ls6jlktH3cZrPp3Vdnq4xPWUXFLNKYWUtVt2kLLZ42Rinnz7qwYqDocmlDMXbsWNWuXVtJSUk6duyYypQpo2bNmikuLi7X54iOjlZKSorTNnTEmHysGkaVLuOlCiGV9Pup6/9zrXVnHUnS6d9y/88duBUFBlfU1LlLtWrdTr32znr9Z9F/dSX7igKCbpPP/18omvzHeaf3pPxx3r4PwJ+uNhPnzyRq2OR59nRCko4dOqDD+3fp8VFTVLVWXYVUraHHBo9ScQ+rdm/51IVVA0WXSxuKXbt2KSYmRuXKlVO1atX0ySefKDw8XM2bN9fPP+duYZTVapWXl5fTxnSnwuXSxYs6/fsp+ZUrf939J348Jkny9StXkGUBhVaJkiVV1q+80i6k6uC+WDW+p9WfTYWvnw5/vdd+3MX0NP30/XeqEVrXhdUChcvVZiIp/pSemTJPpb28nfZnZWZI+nPqkyOLxSKbLafA6kThZnHh/4oil66huHTpkooV+78SLBaLFi9erKFDh6ply5ZavXq1C6uDWYvnz9I9zVsqIDBYZ8+e0YqlC+Xm5q772nXQ77+d0paN69Xknuby8vbRieM/atG8marboKGqVmcxHP7dvtm3S7JJwRUrKeH3U/rva/N1W0hl3df+AVksFnXu1ksfrFqmoAoh8g8M1ttvLlbZcuV1972tXF06UGAyLl3Umfjf7K/PJZ7WqZ9/lGcZL3mXLael/3lBcSd+1NPjZyonJ0cp/3+NkWdpLxUrXly316ytUp5l9N/509SxxwAV97Dqq8//p3NJ8ard6B5XfSygSHNpQ1GzZk3t379ftWrVchp/5ZVXJEkPPvigK8rCP3Q2KVHTxo9VakqyvH3Kqk69u/TKslXyKeurrKwsHdi3Wx++85YuZVySv3+gWrS+X30GDHJ12YDLXUxP06rXX9G5s0kqXcZLTZu3Ua/Hn1axYsUlSV17Rigj45KWzHlR6WkXVLNOfY2PeZlnUOBfJe74D5o3bpj99YdvvCxJanpfB3XqOVCH9u6UJE1/tr/T+56d9rLuqHOXSnv5aOjE2frfW69p/vhnlH3lioJCqmjw8zNUoUr1AvscKNx4UrYxFpvNZnPVxWNiYvTll19qw4YN193/9NNPa8mSJcrJMRZB/p6clRflAf86f6TxswOYkZiW4eoSgCKnTc3CO9X5WMJFl127RmApl13bLJc2FPmFhgIwh4YCMIeGAjCuMDcUP7qwobijCDYUPNgOAAAAgGk0FAAAAABMc/mTsgEAAIBChUXZhpBQAAAAADCNhAIAAABwUFQfMOcqJBQAAAAATKOhAAAAAGAaU54AAAAABzwp2xgSCgAAAACmkVAAAAAADggojCGhAAAAAGAaDQUAAAAA05jyBAAAADhizpMhJBQAAAAATCOhAAAAABzwpGxjSCgAAAAAmEZCAQAAADjgwXbGkFAAAAAAMI2GAgAAAIBpTHkCAAAAHDDjyRgSCgAAAACmkVAAAAAAjogoDCGhAAAAAGAaDQUAAAAA05jyBAAAADjgSdnGkFAAAAAAMI2EAgAAAHDAk7KNIaEAAAAAYBoJBQAAAOCAgMIYEgoAAAAAptFQAAAAADCNKU8AAACAAxZlG0NCAQAAAMA0EgoAAADACRGFESQUAAAAAEyjoQAAAABgGlOeAAAAAAcsyjaGhAIAAACAaSQUAAAAgAMCCmNIKAAAAACYRkIBAAAAOGANhTEkFAAAAEARNGnSJFksFqetZs2a9v0ZGRmKjIyUn5+fSpcure7duysxMTHP66ChAAAAAIqoO++8U/Hx8fZt586d9n0jRozQJ598ovfff1/bt2/X6dOn1a1btzyvgSlPAAAAgANLEVqWXaxYMQUGBl4znpKSomXLlmn16tW67777JElvvvmmatWqpd27d6tp06Z5VgMJBQAAAFBIZGZmKjU11WnLzMy84fE//fSTgoODdfvtt6t3796Ki4uTJB04cECXL19W27Zt7cfWrFlTISEhio2NzdOaaSgAAAAARxbXbTExMfL29nbaYmJirltmkyZNtHz5cn322WdavHixTp48qebNm+vChQtKSEiQh4eHfHx8nN4TEBCghISEPPmarmLKEwAAAFBIREdHKyoqymnMarVe99gOHTrY/1y3bl01adJElSpV0nvvvaeSJUvma52OSCgAAACAQsJqtcrLy8tpu1FD8Vc+Pj664447dPz4cQUGBiorK0vJyclOxyQmJl53zcU/QUMBAAAAOHDhjKd/JC0tTSdOnFBQUJAaNmyo4sWLa/Pmzfb9x44dU1xcnMLCwv7hlZwx5QkAAAAogkaNGqUHHnhAlSpV0unTpzVx4kS5u7vrsccek7e3twYOHKioqCj5+vrKy8tLw4YNU1hYWJ7e4UmioQAAAACcFJUnZf/222967LHHdO7cOZUvX1733nuvdu/erfLly0uS5s6dKzc3N3Xv3l2ZmZkKDw/XokWL8rwOi81ms+X5WV3s9+QsV5cAFEl/pPGzA5iRmJbh6hKAIqdNzXKuLuGGki5cdtm1/csUd9m1zSKhAAAAABwUpQfbFQYsygYAAABgGg0FAAAAANOY8gQAAAA4YsaTISQUAAAAAEwjoQAAAAAcEFAYQ0IBAAAAwDQaCgAAAACmMeUJAAAAcFBUnpRdWJBQAAAAADCNhAIAAABwwJOyjSGhAAAAAGAaCQUAAADggDUUxpBQAAAAADCNhgIAAACAaTQUAAAAAEyjoQAAAABgGouyAQAAAAcsyjaGhAIAAACAaTQUAAAAAExjyhMAAADggCdlG0NCAQAAAMA0EgoAAADAAYuyjSGhAAAAAGAaCQUAAADggIDCGBIKAAAAAKbRUAAAAAAwjSlPAAAAgCPmPBlCQgEAAADANBIKAAAAwAEPtjOGhAIAAACAaTQUAAAAAExjyhMAAADggCdlG0NCAQAAAMA0EgoAAADAAQGFMSQUAAAAAEyjoQAAAABgGlOeAAAAAEfMeTKEhAIAAACAaSQUAAAAgAOelG0MCQUAAAAA00goAAAAAAc82M4YEgoAAAAAptFQAAAAADDNYrPZbK4uAv8emZmZiomJUXR0tKxWq6vLAYoEfm4Ac/jZAQoGDQUKVGpqqry9vZWSkiIvLy9XlwMUCfzcAObwswMUDKY8AQAAADCNhgIAAACAaTQUAAAAAEyjoUCBslqtmjhxIovjAAP4uQHM4WcHKBgsygYAAABgGgkFAAAAANNoKAAAAACYRkMBAAAAwDQaCgAAAACm0VCgwCxcuFCVK1dWiRIl1KRJE+3du9fVJQGF2o4dO/TAAw8oODhYFotFa9eudXVJQJEQExOjxo0bq0yZMvL391fXrl117NgxV5cF3LJoKFAg3n33XUVFRWnixIn6+uuvVa9ePYWHhyspKcnVpQGFVnp6uurVq6eFCxe6uhSgSNm+fbsiIyO1e/dubdq0SZcvX1a7du2Unp7u6tKAWxK3jUWBaNKkiRo3bqxXXnlFkpSTk6OKFStq2LBheu6551xcHVD4WSwWrVmzRl27dnV1KUCRc+bMGfn7+2v79u1q0aKFq8sBbjkkFMh3WVlZOnDggNq2bWsfc3NzU9u2bRUbG+vCygAA/wYpKSmSJF9fXxdXAtyaaCiQ786ePavs7GwFBAQ4jQcEBCghIcFFVQEA/g1ycnL07LPPqlmzZqpdu7arywFuScVcXQAAAEB+iYyM1HfffaedO3e6uhTglkVDgXxXrlw5ubu7KzEx0Wk8MTFRgYGBLqoKAHCrGzp0qNatW6cdO3aoQoUKri4HuGUx5Qn5zsPDQw0bNtTmzZvtYzk5Odq8ebPCwsJcWBkA4FZks9k0dOhQrVmzRlu2bFGVKlVcXRJwSyOhQIGIiopSRESEGjVqpLvvvlvz5s1Tenq6BgwY4OrSgEIrLS1Nx48ft78+efKkDh48KF9fX4WEhLiwMqBwi4yM1OrVq/Xxxx+rTJky9vV63t7eKlmypIurA2493DYWBeaVV17RSy+9pISEBNWvX18LFixQkyZNXF0WUGht27ZNrVu3vmY8IiJCy5cvL/iCgCLCYrFcd/zNN99U//79C7YY4F+AhgIAAACAaayhAAAAAGAaDQUAAAAA02goAAAAAJhGQwEAAADANBoKAAAAAKbRUAAAAAAwjYYCAAAAgGk0FAAAAABMo6EAgH+of//+6tq1q/11q1at9OyzzxZ4Hdu2bZPFYlFycnK+XeOvn9WMgqgTAFBwaCgA3JL69+8vi8Uii8UiDw8PVatWTVOmTNGVK1fy/dofffSRpk6dmqtjC/ov15UrV9a8efMK5FoAgH+HYq4uAADyS/v27fXmm28qMzNTGzZsUGRkpIoXL67o6Ohrjs3KypKHh0eeXNfX1zdPzgMAQFFAQgHglmW1WhUYGKhKlSppyJAhatu2rf73v/9J+r+pOy+++KKCg4NVo0YNSdKpU6f06KOPysfHR76+vurSpYt++eUX+zmzs7MVFRUlHx8f+fn5acyYMbLZbE7X/euUp8zMTI0dO1YVK1aU1WpVtWrVtGzZMv3yyy9q3bq1JKls2bKyWCzq37+/JCknJ0cxMTGqUqWKSpYsqXr16umDDz5wus6GDRt0xx13qGTJkmrdurVTnWZkZ2dr4MCB9mvWqFFD8+fPv+6xkydPVvny5eXl5aXBgwcrKyvLvi83tQMAbh0kFAD+NUqWLKlz587ZX2/evFleXl7atGmTJOny5csKDw9XWFiYvvzySxUrVkzTpk1T+/btdejQIXl4eGj27Nlavny53njjDdWqVUuzZ8/WmjVrdN99993wuv369VNsbKwWLFigevXq6eTJkzp79qwqVqyoDz/8UN27d9exY8fk5eWlkiVLSpJiYmL01ltvacmSJapevbp27NihPn36qHz58mrZsqVOnTqlbt26KTIyUoMGDdL+/fs1cuTIf/T95OTkqEKFCnr//ffl5+enXbt2adCgQQoKCtKjjz7q9L2VKFFC27Zt0y+//KIBAwbIz89PL774Yq5qBwDcYmwAcAuKiIiwdenSxWaz2Ww5OTm2TZs22axWq23UqFH2/QEBAbbMzEz7e1auXGmrUaOGLScnxz6WmZlpK1mypG3jxo02m81mCwoKss2cOdO+//Lly7YKFSrYr2Wz2WwtW7a0DR8+3Gaz2WzHjh2zSbJt2rTpunVu3brVJsn2xx9/2McyMjJspUqVsu3atcvp2IEDB9oee+wxm81ms0VHR9tCQ0Od9o8dO/aac/1VpUqVbHPnzr3h/r+KjIy0de/e3f46IiLC5uvra0tPT7ePLV682Fa6dGlbdnZ2rmq/3mcGABRdJBQAblnr1q1T6dKldfnyZeXk5KhXr16aNGmSfX+dOnWc1k18++23On78uMqUKeN0noyMDJ04cUIpKSmKj49XkyZN7PuKFSumRo0aXTPt6aqDBw/K3d3d0G/mjx8/rosXL+r+++93Gs/KylKDBg0kSd9//71THZIUFhaW62vcyMKFC/XGG28oLi5Oly5dUlZWlurXr+90TL169VSqVCmn66alpenUqVNKS0u7ae0AgFsLDQWAW1br1q21ePFieXh4KDg4WMWKOf8rz9PT0+l1WlqaGjZsqFWrVl1zrvLly5uq4eoUJiPS0tIkSevXr9dtt93mtM9qtZqqIzfeeecdjRo1SrNnz1ZYWJjKlCmjl156SXv27Mn1OVxVOwDAdWgoANyyPD09Va1atVwff9ddd+ndd9+Vv7+/vLy8rntMUFCQ9uzZoxYtWkiSrly5ogMHDuiuu+667vF16tRRTk6Otm/frrZt216z/2pCkp2dbR8LDQ2V1WpVXFzcDZONWrVq2ReYX7V79+6bf8i/8dVXX+mee+7R008/bR87ceLENcd9++23unTpkr1Z2r17t0qXLq2KFSvK19f3prUDAG4t3OUJAP6/3r17q1y5curSpYu+/PJLnTx5Utu2bdMzzzyj3377TZI0fPhwzZgxQ2vXrtUPP/ygp59++m+fIVG5cmVFRETo8ccf19q1a+3nfO+99yRJlSpVksVi0bp163TmzBmlpaWpTJkyGjVqlEaMGKEVK1boxIkT+vrrr/Xyyy9rxYoVkqTBgwfrp59+0ujRo3Xs2DGtXr1ay5cvz9Xn/P3333Xw4EGn7Y8//lD16tW1f/9+bdy4UT/++KPGjx+vffv2XfP+rKwsDRw4UEePHtWGDRs0ceJEDR06VG5ubrmqHQBwa6GhAID/r1SpUtqxY4dCQkLUrVs31apVSwMHDlRGRoY9sRg5cqT69u2riIgI+7Sghx566G/Pu3jxYj388MN6+umnVbNmTT355JNKT0+XJN12222aPHmynnvuOQUEBGjo0KGSpKlTp2r8+PGKiYlRrVq11L59e61fv15VqlSRJIWEhOjDDz/U2rVrVa9ePS1ZskTTp0/P1eecNWuWGjRo4LStX79eTz31lLp166YePXqoSZMmOnfunFNacVWbNm1UvXp1tWjRQj169NCDDz7otDblZrUDAG4tFtuNVhICAAAAwE2QUAAAAAAwjYYCAAAAgGk0FAAAAABMo6EAAAAAYBoNBQAAAADTaCgAAAAAmEZDAQAAAMA0GgoAAAAAptFQAAAAADCNhgIAAACAaTQUAAAAAEz7f9FzbJncFvunAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example Predictions:\n",
            "\n",
            "Example 1 [✓]\n",
            "Text: [CLS] operating loss totalled eur 0. 3 mn, down from a profit of eur 5. 1 mn in the first half of 2009. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "True Label: 0\n",
            "Predicted: Negative (Confidence: 98.30%)\n",
            "\n",
            "Example 2 [✓]\n",
            "Text: [CLS] the operating margin of aker yards cruise & ferries division went down from 8. 3 % to 6. 4 % in the first quarter of 2007. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "True Label: 0\n",
            "Predicted: Negative (Confidence: 98.23%)\n",
            "\n",
            "Example 3 [✓]\n",
            "Text: [CLS] pretax loss totaled eur 1. 2 mn, down from a profit of eur 2. 1 mn in 2004. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "True Label: 0\n",
            "Predicted: Negative (Confidence: 97.80%)\n",
            "\n",
            "Example 4 [✓]\n",
            "Text: [CLS] in the baltic states the company reports net sales of eur 11. 9 mn, down from eur 14. 2 mn, and an operative ebit of eur - 2. 2 mn, down from eur - 1. 7 mn. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "True Label: 0\n",
            "Predicted: Negative (Confidence: 97.66%)\n",
            "\n",
            "Example 5 [✓]\n",
            "Text: [CLS] operating profit, excluding non - recurring items, totalled eur 2. 2 mn, down from eur 2. 7 mn in the corresponding period in 2008. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "True Label: 0\n",
            "Predicted: Negative (Confidence: 97.61%)\n",
            "\n",
            "Example 6 [✓]\n",
            "Text: [CLS] the group ' s operating loss was eur 0. 8 mn, down from a profit of eur 2. 5 mn in 2004. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "True Label: 0\n",
            "Predicted: Negative (Confidence: 97.48%)\n",
            "\n",
            "Example 7 [✓]\n",
            "Text: [CLS] the finnish national carrier said net loss in april through june was euro26 million, down from a net profit of euro13 million a year earlier. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "True Label: 0\n",
            "Predicted: Negative (Confidence: 97.03%)\n",
            "\n",
            "Example 8 [✓]\n",
            "Text: [CLS] total value of the contract is about eur 10mn. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "True Label: 1\n",
            "Predicted: Neutral (Confidence: 96.30%)\n",
            "\n",
            "Example 9 [✓]\n",
            "Text: [CLS] pretax loss totalled eur 49. 9 mn, compared to a loss of eur 15. 4 mn in the corresponding period in 2008. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "True Label: 0\n",
            "Predicted: Negative (Confidence: 96.25%)\n",
            "\n",
            "Example 10 [✓]\n",
            "Text: [CLS] operating profit decreased to nearly eur 1. 7 mn, however. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "True Label: 0\n",
            "Predicted: Negative (Confidence: 95.75%)\n",
            "\n",
            "Negative Class Accuracy: 0.6379\n",
            "\n",
            "Neutral Class Accuracy: 0.7022\n",
            "\n",
            "Positive Class Accuracy: 0.4689\n",
            "\n",
            "Overall Accuracy: 0.6289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " def predict_sentiment(model, tokenizer, sentence, device):\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    inputs = tokenizer(\n",
        "        sentence,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Move inputs to device\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Get probability scores\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        confidence = torch.max(probabilities).item()\n",
        "\n",
        "    # Convert prediction to sentiment\n",
        "    sentiment_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "    predicted_sentiment = sentiment_map[predicted.item()]\n",
        "\n",
        "    return predicted_sentiment, confidence"
      ],
      "metadata": {
        "id": "s0LI1N0gQ_FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the Model with a Sentence"
      ],
      "metadata": {
        "id": "5v_VDXhFZfAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Earnings are declining 0.5 percent compared to last quarter\"\n",
        "\n",
        "sentiment, confidence = predict_sentiment(model, tokenizer, sentence, device)\n",
        "\n",
        "print(f\"Sentnence: {sentence}\\nSentiment: {sentiment}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9sfz0KMYZDp",
        "outputId": "a43f634e-776b-4663-e682-7814e82bb558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentnence: Earnings are declining 0.5 percent compared to last quarter\n",
            "Sentiment: Negative\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}